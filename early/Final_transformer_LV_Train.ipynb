{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO1WOlEs+sdqwW3Zr2eRDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fyzhang1/Final_Project/blob/main/Final_transformer_LV_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LV动态系统模型的定义"
      ],
      "metadata": {
        "id": "gYmQKLNgCRTB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhB3QtyDfm5R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.integrate import solve_ivp\n",
        "from functools import partial\n",
        "from statistics import mean\n",
        "\n",
        "class LotkaVolterraDataset(Dataset):\n",
        "\n",
        "    def __init__(self, num_traj_per_env, time_horizon, params, dt, batch_t=10, method='RK45', group='train'):\n",
        "        super().__init__()\n",
        "        self.num_traj_per_env = num_traj_per_env\n",
        "        self.num_env = len(params)\n",
        "        self.len = num_traj_per_env * self.num_env\n",
        "        self.time_horizon = float(time_horizon)       # total time\n",
        "        self.dt = dt\n",
        "        self.batch_t = batch_t\n",
        "\n",
        "        self.params_eq = params\n",
        "        self.test = group == 'test'\n",
        "        self.max = np.iinfo(np.int32).max\n",
        "        self.buffer = dict()\n",
        "        self.method = method\n",
        "        self.indices = [list(range(env * num_traj_per_env, (env + 1) * num_traj_per_env)) for env in range(self.num_env)]\n",
        "\n",
        "    def _f(self, t, x, env=0):\n",
        "        alpha = self.params_eq[env]['alpha']\n",
        "        beta  = self.params_eq[env]['beta' ]\n",
        "        gamma = self.params_eq[env]['gamma']\n",
        "        delta = self.params_eq[env]['delta']\n",
        "\n",
        "        d = np.zeros(2)\n",
        "        d[0] = alpha * x[0] - beta * x[0] * x[1]\n",
        "        d[1] = delta * x[0] * x[1] - gamma * x[1]\n",
        "        return d\n",
        "\n",
        "    def _get_init_cond(self, index):\n",
        "        np.random.seed(index if not self.test else self.max-index)\n",
        "        return np.random.random(2) + 1.\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        env       = index // self.num_traj_per_env\n",
        "        env_index = index %  self.num_traj_per_env\n",
        "        t = torch.arange(0, self.time_horizon, self.dt).float()\n",
        "        t0 = torch.randint(t.size(0) - self.batch_t + 1, (1,)).item()\n",
        "        if self.buffer.get(index) is None:\n",
        "            y0 = self._get_init_cond(env_index)\n",
        "\n",
        "            res = solve_ivp(partial(self._f, env=env), (0., self.time_horizon), y0=y0, method=self.method, t_eval=np.arange(0., self.time_horizon, self.dt))\n",
        "            res = torch.from_numpy(res.y).float()\n",
        "            self.buffer[index] = res.numpy()\n",
        "            return {\n",
        "                'state'   : res,\n",
        "                't'       : t,\n",
        "                'env'     : env,\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'state'   : torch.from_numpy(self.buffer[index]),\n",
        "                't'       : t,\n",
        "                'env'     : env,\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from torch.utils.data.sampler import Sampler\n",
        "import random\n",
        "\n",
        "class SubsetSequentialSampler(Sampler):\n",
        "    def __init__(self, indices, mini_batch_size):\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        if not any(isinstance(el, list) for el in indices):\n",
        "            self.indices = [indices]\n",
        "        else:\n",
        "            self.indices = indices\n",
        "        self.env_len = len(self.indices[0])\n",
        "\n",
        "    def __iter__(self):\n",
        "        if len(self.indices) > 1:\n",
        "            l_indices = copy.deepcopy(self.indices)\n",
        "\n",
        "            l_iter = list()\n",
        "            for _ in range(0, self.env_len, self.mini_batch_size):\n",
        "                for i in range(len(l_indices)):\n",
        "                    l_iter.extend(l_indices[i][:self.mini_batch_size])\n",
        "                    del l_indices[i][:self.mini_batch_size]\n",
        "        else:\n",
        "            l_iter = copy.deepcopy(self.indices[0])\n",
        "        return iter(l_iter)\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum([len(el) for el in self.indices])\n",
        "\n",
        "class SubsetRamdomSampler(Sampler):\n",
        "    def __init__(self, indices, mini_batch_size, same_order_in_groups=True):\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.same_order_in_groups = same_order_in_groups\n",
        "        if not any(isinstance(el, list) for el in indices):\n",
        "            self.indices = [indices]\n",
        "        else:\n",
        "            self.indices = indices\n",
        "        self.env_len = len(self.indices[0])\n",
        "\n",
        "    def __iter__(self):\n",
        "        if len(self.indices) > 1:\n",
        "            if self.same_order_in_groups:\n",
        "                l_shuffled = copy.deepcopy(self.indices)\n",
        "                random.shuffle(l_shuffled[0])\n",
        "                for i in range(1, len(self.indices)):\n",
        "                    l_shuffled[i] = [el + i * self.env_len for el in l_shuffled[0]]\n",
        "            else:\n",
        "                l_shuffled = copy.deepcopy(self.indices)\n",
        "                for l in l_shuffled:\n",
        "                    random.shuffle(l)\n",
        "\n",
        "            l_iter = list()\n",
        "            for _ in range(0, self.env_len, self.mini_batch_size):\n",
        "                for i in range(len(l_shuffled)):\n",
        "                    l_iter.extend(l_shuffled[i][:self.mini_batch_size])\n",
        "                    del l_shuffled[i][:self.mini_batch_size]\n",
        "        else:\n",
        "            l_shuffled = copy.deepcopy(self.indices[0])\n",
        "            random.shuffle(l_shuffled)\n",
        "            l_iter = l_shuffled\n",
        "        return iter(l_iter)\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum([len(el) for el in self.indices])"
      ],
      "metadata": {
        "id": "QK1KxgdaftpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.integrate import solve_ivp\n",
        "from functools import partial\n",
        "from statistics import mean\n",
        "from scipy.stats import ortho_group\n",
        "\n",
        "class LinearDataset(Dataset):\n",
        "\n",
        "    def __init__(self, num_traj_per_env, time_horizon, params, dt, method='RK45', group='train'):\n",
        "        super().__init__()\n",
        "        self.num_traj_per_env = num_traj_per_env\n",
        "        self.params_eq = params\n",
        "        self.num_env = len(params)\n",
        "        self.dim = len(params[0]['eig_vals'])\n",
        "        self.len = num_traj_per_env * self.num_env\n",
        "        self.time_horizon = float(time_horizon)\n",
        "        self.dt = dt\n",
        "\n",
        "        np.random.seed(19700101)\n",
        "        self.mat_ortho = ortho_group.rvs(self.dim)\n",
        "\n",
        "        self.test = group == 'test'\n",
        "        self.max = np.iinfo(np.int32).max\n",
        "        self.buffer = dict()\n",
        "        self.method = method\n",
        "        self.indices = [list(range(env * num_traj_per_env, (env + 1) * num_traj_per_env)) for env in range(self.num_env)]\n",
        "\n",
        "    def _f(self, t, x, env=0):\n",
        "        eig_vals = np.array(self.params_eq[env]['eig_vals'])\n",
        "        b = self.params_eq[env].get('b')\n",
        "        sigma = np.diag(eig_vals)\n",
        "        deriv = self.mat_ortho.T @ sigma @ self.mat_ortho @ x\n",
        "        if b is not None:\n",
        "            deriv += b\n",
        "        return deriv\n",
        "\n",
        "    def _get_init_cond(self, index):\n",
        "        np.random.seed(index if not self.test else self.max-index)\n",
        "        return np.random.randn(self.dim)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        env       = index // self.num_traj_per_env\n",
        "        env_index = index %  self.num_traj_per_env\n",
        "        t = torch.arange(0, self.time_horizon, self.dt).float()\n",
        "        if self.buffer.get(index) is None:\n",
        "            u_0 = self._get_init_cond(env_index)\n",
        "\n",
        "            res = solve_ivp(partial(self._f, env=env), (0., self.time_horizon), y0=u_0, method=self.method, t_eval=np.arange(0., self.time_horizon, self.dt))\n",
        "            res_u = torch.from_numpy(res.y).float()\n",
        "\n",
        "            self.buffer[index] = res_u.numpy()\n",
        "            return {\n",
        "                'state'   : res_u,\n",
        "                't'       : t,\n",
        "                'env'     : env,\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'state'   : torch.from_numpy(self.buffer[index]),\n",
        "                't'       : t,\n",
        "                'env'     : env,\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "rgZ_oT40fyzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def param_lv():\n",
        "    params = [\n",
        "        {'alpha': 0.5 , 'beta': 0.5, 'gamma': 0.5 , 'delta': 0.5},\n",
        "        {'alpha': 0.25, 'beta': 0.5, 'gamma': 0.5 , 'delta': 0.5},\n",
        "        {'alpha': 0.75, 'beta': 0.5, 'gamma': 0.5 , 'delta': 0.5},\n",
        "        {'alpha': 1.0 , 'beta': 0.5, 'gamma': 0.5 , 'delta': 0.5},\n",
        "        {'alpha': 0.5 , 'beta': 0.5, 'gamma': 0.25, 'delta': 0.5},\n",
        "        {'alpha': 0.5 , 'beta': 0.5, 'gamma': 0.75, 'delta': 0.5},\n",
        "        {'alpha': 0.5 , 'beta': 0.5, 'gamma': 1.0 , 'delta': 0.5},\n",
        "        {'alpha': 0.25, 'beta': 0.5, 'gamma': 0.25, 'delta': 0.5},\n",
        "        {'alpha': 0.75, 'beta': 0.5, 'gamma': 0.75, 'delta': 0.5},\n",
        "        {'alpha': 1.0 , 'beta': 0.5, 'gamma': 1.0 , 'delta': 0.5},\n",
        "    ]\n",
        "\n",
        "    n_env = len(params)\n",
        "    mini_batch_size = 1\n",
        "\n",
        "    dataset_train_params = {\n",
        "        'num_traj_per_env': 1,\n",
        "        'time_horizon': 10,\n",
        "        'params': params,\n",
        "        'dt': 0.5,\n",
        "        'method': 'RK45',\n",
        "        'group': 'train',\n",
        "    }\n",
        "\n",
        "    dataset_test_params = dict()\n",
        "    dataset_test_params.update(dataset_train_params)\n",
        "    dataset_test_params['num_traj_per_env'] = 32\n",
        "    dataset_test_params['group'] = 'test'\n",
        "\n",
        "    dataset_train = LotkaVolterraDataset(**dataset_train_params)\n",
        "    dataset_test  = LotkaVolterraDataset(**dataset_test_params)\n",
        "    sampler_train = SubsetRamdomSampler(indices=dataset_train.indices, mini_batch_size=mini_batch_size)\n",
        "    sampler_test  = SubsetSequentialSampler(indices=dataset_test.indices , mini_batch_size=1)\n",
        "\n",
        "    dataloader_train_params = {\n",
        "        'dataset'    : dataset_train,\n",
        "        'batch_size' : mini_batch_size * n_env,\n",
        "        'num_workers': 0,\n",
        "        'sampler'    : sampler_train,\n",
        "        'pin_memory' : True,\n",
        "        'drop_last'  : False,\n",
        "    }\n",
        "\n",
        "    dataloader_test_params = {\n",
        "        'dataset'    : dataset_test,\n",
        "        'batch_size' : n_env,\n",
        "        'num_workers': 0,\n",
        "        'sampler'    : sampler_test,\n",
        "        'pin_memory' : True,\n",
        "        'drop_last'  : False,\n",
        "    }\n",
        "\n",
        "    dataloader_train = DataLoader(**dataloader_train_params)\n",
        "    dataloader_test  = DataLoader(**dataloader_test_params)\n",
        "\n",
        "    return dataloader_train, dataloader_test\n",
        "\n",
        "\n",
        "def init_dataloaders(dataset, buffer_filepath=None):\n",
        "    if dataset == 'lv':\n",
        "        return param_lv()"
      ],
      "metadata": {
        "id": "5Y3JaeaWf3qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = init_dataloaders('lv')"
      ],
      "metadata": {
        "id": "lVKOY4FxgAa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 获取一个批次的数据\n",
        "train_batch = next(iter(train_data))\n",
        "\n",
        "# 查看 train_batch 的键和值的类型\n",
        "print(\"Train batch keys:\", train_batch.keys())\n",
        "\n",
        "# 查看字典中每个键对应的值的形状\n",
        "for key in train_batch:\n",
        "    print(f\"Key: {key}, Shape: {train_batch[key].shape}\")\n",
        "\n",
        "# 使用 'state' 键提取输入数据\n",
        "input_data = train_batch['state']  # 输入数据，形状为 [batch_size, seq_len, feature_dim]\n",
        "\n",
        "# 'state' 的形状为 [batch_size, feature_dim, seq_len]，需要调整顺序以便绘图\n",
        "input_data = input_data.permute(0, 2, 1)  # 变为 [batch_size, seq_len, feature_dim]\n",
        "\n",
        "# 选择一个样本进行可视化（例如，选择第一个样本）\n",
        "sample_index = 0\n",
        "time_steps = range(input_data.shape[1])\n",
        "\n",
        "# 提取捕食者和被捕食者数量\n",
        "prey_population = input_data[sample_index, :, 0].numpy()  # 被捕食者\n",
        "predator_population = input_data[sample_index, :, 1].numpy()  # 捕食者\n",
        "\n",
        "# 可视化捕食者和被捕食者的数量\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(time_steps, prey_population, label=\"Prey Population\", color='blue')\n",
        "plt.plot(time_steps, predator_population, label=\"Predator Population\", color='red')\n",
        "\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Population')\n",
        "plt.title('Lotka-Volterra Training Data Sample')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "A2MxL5YSgipg",
        "outputId": "35794c8d-5a38-45bd-8b77-986ae0db6379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batch keys: dict_keys(['state', 't', 'env'])\n",
            "Key: state, Shape: torch.Size([10, 2, 20])\n",
            "Key: t, Shape: torch.Size([10, 20])\n",
            "Key: env, Shape: torch.Size([10])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "XViXcEAtCW74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# 3. 定义 Transformer 模型\n",
        "class Configs:\n",
        "    def __init__(self):\n",
        "        self.label_len = 10\n",
        "        self.pred_len = 1\n",
        "        self.enc_in = 2  # 输入特征维度\n",
        "        self.dec_in = 2  # 解码器输入特征维度\n",
        "        self.d_model = 64\n",
        "        self.e_layers = 2  # 编码器层数\n",
        "        self.d_layers = 1  # 解码器层数\n",
        "        self.n_heads = 4  # 注意力头数\n",
        "        self.d_ff = 128  # FeedForward 层维度\n",
        "        self.dropout = 0.1\n",
        "        self.activation = \"gelu\"\n",
        "        self.output_attention = False\n",
        "        self.rev = False\n",
        "        self.c_out = 2  # 输出维度\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, dropout):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "        self.value_embedding = nn.Linear(input_dim, d_model)\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, 5000, d_model))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark=None):\n",
        "        x = self.value_embedding(x) + self.position_embedding[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.self_attention(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, e_layers, n_heads, d_model, d_ff, dropout, activation, output_attention, norm_layer=None):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(e_layers)])\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x, None\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.cross_attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out):\n",
        "        self_attn_output, _ = self.self_attention(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "        cross_attn_output, _ = self.cross_attention(x, enc_out, enc_out)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_layers, n_heads, d_model, d_ff, dropout, activation, output_attention, norm_layer=None):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(d_layers)])\n",
        "\n",
        "    def forward(self, x, enc_out, x_mask=None, cross_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out)\n",
        "        return x\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(Model, self).__init__()\n",
        "        self.label_len = configs.label_len\n",
        "        self.pred_len = configs.pred_len\n",
        "        self.output_attention = configs.output_attention\n",
        "\n",
        "        # Embedding\n",
        "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.dropout)\n",
        "        self.dec_embedding = DataEmbedding(configs.dec_in, configs.d_model, configs.dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(\n",
        "            configs.e_layers, configs.n_heads, configs.d_model, configs.d_ff,\n",
        "            configs.dropout, configs.activation, configs.output_attention,\n",
        "            norm_layer=torch.nn.LayerNorm(configs.d_model)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(\n",
        "            configs.d_layers, configs.n_heads, configs.d_model, configs.d_ff,\n",
        "            configs.dropout, configs.activation, configs.output_attention,\n",
        "            norm_layer=torch.nn.LayerNorm(configs.d_model),\n",
        "        )\n",
        "        self.projection = nn.Linear(configs.d_model, configs.c_out)\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
        "        enc_out = self.enc_embedding(x_enc)\n",
        "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
        "\n",
        "        dec_out = self.dec_embedding(x_dec)\n",
        "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
        "        dec_out = self.projection(dec_out)\n",
        "\n",
        "        return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
      ],
      "metadata": {
        "id": "IKxzzw9-hhuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8ADG1usTCcyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 训练 Transformer 模型来拟合 LV 系统的时间序列数据\n",
        "configs = Configs()\n",
        "model = Model(configs)\n",
        "\n",
        "# 初始化优化器和损失函数\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 1000"
      ],
      "metadata": {
        "id": "Kh6Ll0GMhmyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练模型\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_rmse = 0.0\n",
        "    num_batches = len(train_data)\n",
        "\n",
        "    for batch in train_data:\n",
        "        # 提取 'state' 作为输入数据\n",
        "        input_data = batch['state'].permute(0, 2, 1)  # 调整形状为 [batch_size, seq_len, feature_dim]\n",
        "        target_data = input_data[:, -1, :]  # 使用最后一个时间步作为目标\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 前向传播\n",
        "        output = model(input_data, None, input_data, None).squeeze()\n",
        "\n",
        "        # 计算损失\n",
        "        loss = criterion(output, target_data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # 计算 RMSE\n",
        "        rmse = torch.sqrt(torch.mean((output - target_data) ** 2))\n",
        "        total_rmse += rmse.item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    avg_rmse = total_rmse / num_batches\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}, RMSE: {avg_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER18bsQhiBBm",
        "outputId": "958d31bb-1ab2-4cd0-8b6a-dafd6e3e05db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 1.9310, RMSE: 1.3896\n",
            "Epoch [2/1000], Loss: 1.0212, RMSE: 1.0105\n",
            "Epoch [3/1000], Loss: 0.8545, RMSE: 0.9244\n",
            "Epoch [4/1000], Loss: 0.7250, RMSE: 0.8515\n",
            "Epoch [5/1000], Loss: 0.7268, RMSE: 0.8525\n",
            "Epoch [6/1000], Loss: 0.6018, RMSE: 0.7758\n",
            "Epoch [7/1000], Loss: 0.5347, RMSE: 0.7312\n",
            "Epoch [8/1000], Loss: 0.3550, RMSE: 0.5958\n",
            "Epoch [9/1000], Loss: 0.3685, RMSE: 0.6070\n",
            "Epoch [10/1000], Loss: 0.2755, RMSE: 0.5249\n",
            "Epoch [11/1000], Loss: 0.3751, RMSE: 0.6124\n",
            "Epoch [12/1000], Loss: 0.3441, RMSE: 0.5866\n",
            "Epoch [13/1000], Loss: 0.2658, RMSE: 0.5156\n",
            "Epoch [14/1000], Loss: 0.1941, RMSE: 0.4406\n",
            "Epoch [15/1000], Loss: 0.1616, RMSE: 0.4020\n",
            "Epoch [16/1000], Loss: 0.1263, RMSE: 0.3554\n",
            "Epoch [17/1000], Loss: 0.1156, RMSE: 0.3401\n",
            "Epoch [18/1000], Loss: 0.1613, RMSE: 0.4016\n",
            "Epoch [19/1000], Loss: 0.1423, RMSE: 0.3773\n",
            "Epoch [20/1000], Loss: 0.1897, RMSE: 0.4356\n",
            "Epoch [21/1000], Loss: 0.0969, RMSE: 0.3113\n",
            "Epoch [22/1000], Loss: 0.1472, RMSE: 0.3837\n",
            "Epoch [23/1000], Loss: 0.0837, RMSE: 0.2893\n",
            "Epoch [24/1000], Loss: 0.0825, RMSE: 0.2873\n",
            "Epoch [25/1000], Loss: 0.0486, RMSE: 0.2205\n",
            "Epoch [26/1000], Loss: 0.1101, RMSE: 0.3318\n",
            "Epoch [27/1000], Loss: 0.1036, RMSE: 0.3219\n",
            "Epoch [28/1000], Loss: 0.1278, RMSE: 0.3574\n",
            "Epoch [29/1000], Loss: 0.1185, RMSE: 0.3443\n",
            "Epoch [30/1000], Loss: 0.1284, RMSE: 0.3583\n",
            "Epoch [31/1000], Loss: 0.0687, RMSE: 0.2621\n",
            "Epoch [32/1000], Loss: 0.0766, RMSE: 0.2767\n",
            "Epoch [33/1000], Loss: 0.1008, RMSE: 0.3175\n",
            "Epoch [34/1000], Loss: 0.1355, RMSE: 0.3682\n",
            "Epoch [35/1000], Loss: 0.1050, RMSE: 0.3240\n",
            "Epoch [36/1000], Loss: 0.0843, RMSE: 0.2904\n",
            "Epoch [37/1000], Loss: 0.0729, RMSE: 0.2700\n",
            "Epoch [38/1000], Loss: 0.0930, RMSE: 0.3049\n",
            "Epoch [39/1000], Loss: 0.0900, RMSE: 0.3000\n",
            "Epoch [40/1000], Loss: 0.0419, RMSE: 0.2048\n",
            "Epoch [41/1000], Loss: 0.0752, RMSE: 0.2742\n",
            "Epoch [42/1000], Loss: 0.0697, RMSE: 0.2640\n",
            "Epoch [43/1000], Loss: 0.0431, RMSE: 0.2075\n",
            "Epoch [44/1000], Loss: 0.0586, RMSE: 0.2421\n",
            "Epoch [45/1000], Loss: 0.0621, RMSE: 0.2492\n",
            "Epoch [46/1000], Loss: 0.0567, RMSE: 0.2381\n",
            "Epoch [47/1000], Loss: 0.0413, RMSE: 0.2031\n",
            "Epoch [48/1000], Loss: 0.0854, RMSE: 0.2922\n",
            "Epoch [49/1000], Loss: 0.0783, RMSE: 0.2798\n",
            "Epoch [50/1000], Loss: 0.0723, RMSE: 0.2688\n",
            "Epoch [51/1000], Loss: 0.0345, RMSE: 0.1858\n",
            "Epoch [52/1000], Loss: 0.0401, RMSE: 0.2002\n",
            "Epoch [53/1000], Loss: 0.0453, RMSE: 0.2128\n",
            "Epoch [54/1000], Loss: 0.0723, RMSE: 0.2688\n",
            "Epoch [55/1000], Loss: 0.0786, RMSE: 0.2803\n",
            "Epoch [56/1000], Loss: 0.0688, RMSE: 0.2624\n",
            "Epoch [57/1000], Loss: 0.0640, RMSE: 0.2530\n",
            "Epoch [58/1000], Loss: 0.0459, RMSE: 0.2142\n",
            "Epoch [59/1000], Loss: 0.0570, RMSE: 0.2388\n",
            "Epoch [60/1000], Loss: 0.0814, RMSE: 0.2853\n",
            "Epoch [61/1000], Loss: 0.0505, RMSE: 0.2246\n",
            "Epoch [62/1000], Loss: 0.0573, RMSE: 0.2394\n",
            "Epoch [63/1000], Loss: 0.0568, RMSE: 0.2384\n",
            "Epoch [64/1000], Loss: 0.0246, RMSE: 0.1568\n",
            "Epoch [65/1000], Loss: 0.0546, RMSE: 0.2337\n",
            "Epoch [66/1000], Loss: 0.0376, RMSE: 0.1939\n",
            "Epoch [67/1000], Loss: 0.0348, RMSE: 0.1866\n",
            "Epoch [68/1000], Loss: 0.0414, RMSE: 0.2034\n",
            "Epoch [69/1000], Loss: 0.0677, RMSE: 0.2601\n",
            "Epoch [70/1000], Loss: 0.0496, RMSE: 0.2227\n",
            "Epoch [71/1000], Loss: 0.0453, RMSE: 0.2128\n",
            "Epoch [72/1000], Loss: 0.0461, RMSE: 0.2148\n",
            "Epoch [73/1000], Loss: 0.0626, RMSE: 0.2501\n",
            "Epoch [74/1000], Loss: 0.0241, RMSE: 0.1551\n",
            "Epoch [75/1000], Loss: 0.0226, RMSE: 0.1504\n",
            "Epoch [76/1000], Loss: 0.0486, RMSE: 0.2205\n",
            "Epoch [77/1000], Loss: 0.0326, RMSE: 0.1804\n",
            "Epoch [78/1000], Loss: 0.0592, RMSE: 0.2434\n",
            "Epoch [79/1000], Loss: 0.0358, RMSE: 0.1892\n",
            "Epoch [80/1000], Loss: 0.0235, RMSE: 0.1534\n",
            "Epoch [81/1000], Loss: 0.0234, RMSE: 0.1531\n",
            "Epoch [82/1000], Loss: 0.0176, RMSE: 0.1328\n",
            "Epoch [83/1000], Loss: 0.0640, RMSE: 0.2529\n",
            "Epoch [84/1000], Loss: 0.0485, RMSE: 0.2203\n",
            "Epoch [85/1000], Loss: 0.0224, RMSE: 0.1498\n",
            "Epoch [86/1000], Loss: 0.0347, RMSE: 0.1864\n",
            "Epoch [87/1000], Loss: 0.0528, RMSE: 0.2299\n",
            "Epoch [88/1000], Loss: 0.0333, RMSE: 0.1825\n",
            "Epoch [89/1000], Loss: 0.0704, RMSE: 0.2654\n",
            "Epoch [90/1000], Loss: 0.0539, RMSE: 0.2322\n",
            "Epoch [91/1000], Loss: 0.0512, RMSE: 0.2263\n",
            "Epoch [92/1000], Loss: 0.0565, RMSE: 0.2377\n",
            "Epoch [93/1000], Loss: 0.0253, RMSE: 0.1591\n",
            "Epoch [94/1000], Loss: 0.0476, RMSE: 0.2182\n",
            "Epoch [95/1000], Loss: 0.0446, RMSE: 0.2112\n",
            "Epoch [96/1000], Loss: 0.0434, RMSE: 0.2084\n",
            "Epoch [97/1000], Loss: 0.0431, RMSE: 0.2076\n",
            "Epoch [98/1000], Loss: 0.0627, RMSE: 0.2505\n",
            "Epoch [99/1000], Loss: 0.0534, RMSE: 0.2310\n",
            "Epoch [100/1000], Loss: 0.0389, RMSE: 0.1971\n",
            "Epoch [101/1000], Loss: 0.0294, RMSE: 0.1714\n",
            "Epoch [102/1000], Loss: 0.0430, RMSE: 0.2075\n",
            "Epoch [103/1000], Loss: 0.0470, RMSE: 0.2167\n",
            "Epoch [104/1000], Loss: 0.0195, RMSE: 0.1398\n",
            "Epoch [105/1000], Loss: 0.0274, RMSE: 0.1657\n",
            "Epoch [106/1000], Loss: 0.0174, RMSE: 0.1321\n",
            "Epoch [107/1000], Loss: 0.0573, RMSE: 0.2395\n",
            "Epoch [108/1000], Loss: 0.0265, RMSE: 0.1627\n",
            "Epoch [109/1000], Loss: 0.0356, RMSE: 0.1886\n",
            "Epoch [110/1000], Loss: 0.0382, RMSE: 0.1954\n",
            "Epoch [111/1000], Loss: 0.0193, RMSE: 0.1390\n",
            "Epoch [112/1000], Loss: 0.0325, RMSE: 0.1803\n",
            "Epoch [113/1000], Loss: 0.0205, RMSE: 0.1431\n",
            "Epoch [114/1000], Loss: 0.0535, RMSE: 0.2312\n",
            "Epoch [115/1000], Loss: 0.0298, RMSE: 0.1727\n",
            "Epoch [116/1000], Loss: 0.0556, RMSE: 0.2357\n",
            "Epoch [117/1000], Loss: 0.0608, RMSE: 0.2465\n",
            "Epoch [118/1000], Loss: 0.0281, RMSE: 0.1677\n",
            "Epoch [119/1000], Loss: 0.0241, RMSE: 0.1552\n",
            "Epoch [120/1000], Loss: 0.0540, RMSE: 0.2324\n",
            "Epoch [121/1000], Loss: 0.0248, RMSE: 0.1574\n",
            "Epoch [122/1000], Loss: 0.0331, RMSE: 0.1819\n",
            "Epoch [123/1000], Loss: 0.0228, RMSE: 0.1509\n",
            "Epoch [124/1000], Loss: 0.0291, RMSE: 0.1707\n",
            "Epoch [125/1000], Loss: 0.0331, RMSE: 0.1820\n",
            "Epoch [126/1000], Loss: 0.0193, RMSE: 0.1389\n",
            "Epoch [127/1000], Loss: 0.0304, RMSE: 0.1745\n",
            "Epoch [128/1000], Loss: 0.0363, RMSE: 0.1906\n",
            "Epoch [129/1000], Loss: 0.0258, RMSE: 0.1607\n",
            "Epoch [130/1000], Loss: 0.0271, RMSE: 0.1646\n",
            "Epoch [131/1000], Loss: 0.0311, RMSE: 0.1763\n",
            "Epoch [132/1000], Loss: 0.0143, RMSE: 0.1194\n",
            "Epoch [133/1000], Loss: 0.0230, RMSE: 0.1518\n",
            "Epoch [134/1000], Loss: 0.0169, RMSE: 0.1301\n",
            "Epoch [135/1000], Loss: 0.0255, RMSE: 0.1597\n",
            "Epoch [136/1000], Loss: 0.0227, RMSE: 0.1508\n",
            "Epoch [137/1000], Loss: 0.0440, RMSE: 0.2098\n",
            "Epoch [138/1000], Loss: 0.0353, RMSE: 0.1879\n",
            "Epoch [139/1000], Loss: 0.0235, RMSE: 0.1532\n",
            "Epoch [140/1000], Loss: 0.0173, RMSE: 0.1316\n",
            "Epoch [141/1000], Loss: 0.0508, RMSE: 0.2253\n",
            "Epoch [142/1000], Loss: 0.0160, RMSE: 0.1265\n",
            "Epoch [143/1000], Loss: 0.0443, RMSE: 0.2105\n",
            "Epoch [144/1000], Loss: 0.0213, RMSE: 0.1459\n",
            "Epoch [145/1000], Loss: 0.0172, RMSE: 0.1313\n",
            "Epoch [146/1000], Loss: 0.0138, RMSE: 0.1173\n",
            "Epoch [147/1000], Loss: 0.0169, RMSE: 0.1300\n",
            "Epoch [148/1000], Loss: 0.0398, RMSE: 0.1995\n",
            "Epoch [149/1000], Loss: 0.0255, RMSE: 0.1596\n",
            "Epoch [150/1000], Loss: 0.0409, RMSE: 0.2023\n",
            "Epoch [151/1000], Loss: 0.0548, RMSE: 0.2340\n",
            "Epoch [152/1000], Loss: 0.0314, RMSE: 0.1771\n",
            "Epoch [153/1000], Loss: 0.0326, RMSE: 0.1807\n",
            "Epoch [154/1000], Loss: 0.0294, RMSE: 0.1714\n",
            "Epoch [155/1000], Loss: 0.0519, RMSE: 0.2278\n",
            "Epoch [156/1000], Loss: 0.0187, RMSE: 0.1366\n",
            "Epoch [157/1000], Loss: 0.0188, RMSE: 0.1371\n",
            "Epoch [158/1000], Loss: 0.0577, RMSE: 0.2402\n",
            "Epoch [159/1000], Loss: 0.0307, RMSE: 0.1752\n",
            "Epoch [160/1000], Loss: 0.0252, RMSE: 0.1587\n",
            "Epoch [161/1000], Loss: 0.0188, RMSE: 0.1372\n",
            "Epoch [162/1000], Loss: 0.0244, RMSE: 0.1561\n",
            "Epoch [163/1000], Loss: 0.0263, RMSE: 0.1622\n",
            "Epoch [164/1000], Loss: 0.0137, RMSE: 0.1170\n",
            "Epoch [165/1000], Loss: 0.0269, RMSE: 0.1641\n",
            "Epoch [166/1000], Loss: 0.0128, RMSE: 0.1133\n",
            "Epoch [167/1000], Loss: 0.0525, RMSE: 0.2291\n",
            "Epoch [168/1000], Loss: 0.0495, RMSE: 0.2225\n",
            "Epoch [169/1000], Loss: 0.0237, RMSE: 0.1538\n",
            "Epoch [170/1000], Loss: 0.0208, RMSE: 0.1443\n",
            "Epoch [171/1000], Loss: 0.0334, RMSE: 0.1827\n",
            "Epoch [172/1000], Loss: 0.0503, RMSE: 0.2242\n",
            "Epoch [173/1000], Loss: 0.0194, RMSE: 0.1394\n",
            "Epoch [174/1000], Loss: 0.0240, RMSE: 0.1548\n",
            "Epoch [175/1000], Loss: 0.0245, RMSE: 0.1566\n",
            "Epoch [176/1000], Loss: 0.0404, RMSE: 0.2011\n",
            "Epoch [177/1000], Loss: 0.0291, RMSE: 0.1707\n",
            "Epoch [178/1000], Loss: 0.0258, RMSE: 0.1605\n",
            "Epoch [179/1000], Loss: 0.0262, RMSE: 0.1618\n",
            "Epoch [180/1000], Loss: 0.0311, RMSE: 0.1763\n",
            "Epoch [181/1000], Loss: 0.0453, RMSE: 0.2127\n",
            "Epoch [182/1000], Loss: 0.0367, RMSE: 0.1916\n",
            "Epoch [183/1000], Loss: 0.0150, RMSE: 0.1223\n",
            "Epoch [184/1000], Loss: 0.0227, RMSE: 0.1506\n",
            "Epoch [185/1000], Loss: 0.0463, RMSE: 0.2152\n",
            "Epoch [186/1000], Loss: 0.0307, RMSE: 0.1751\n",
            "Epoch [187/1000], Loss: 0.0279, RMSE: 0.1670\n",
            "Epoch [188/1000], Loss: 0.0296, RMSE: 0.1721\n",
            "Epoch [189/1000], Loss: 0.0311, RMSE: 0.1763\n",
            "Epoch [190/1000], Loss: 0.0287, RMSE: 0.1695\n",
            "Epoch [191/1000], Loss: 0.0274, RMSE: 0.1657\n",
            "Epoch [192/1000], Loss: 0.0204, RMSE: 0.1427\n",
            "Epoch [193/1000], Loss: 0.0177, RMSE: 0.1332\n",
            "Epoch [194/1000], Loss: 0.0277, RMSE: 0.1666\n",
            "Epoch [195/1000], Loss: 0.0241, RMSE: 0.1553\n",
            "Epoch [196/1000], Loss: 0.0231, RMSE: 0.1520\n",
            "Epoch [197/1000], Loss: 0.0209, RMSE: 0.1446\n",
            "Epoch [198/1000], Loss: 0.0129, RMSE: 0.1137\n",
            "Epoch [199/1000], Loss: 0.0216, RMSE: 0.1468\n",
            "Epoch [200/1000], Loss: 0.0302, RMSE: 0.1737\n",
            "Epoch [201/1000], Loss: 0.0464, RMSE: 0.2154\n",
            "Epoch [202/1000], Loss: 0.0196, RMSE: 0.1400\n",
            "Epoch [203/1000], Loss: 0.0275, RMSE: 0.1658\n",
            "Epoch [204/1000], Loss: 0.0262, RMSE: 0.1619\n",
            "Epoch [205/1000], Loss: 0.0260, RMSE: 0.1611\n",
            "Epoch [206/1000], Loss: 0.0221, RMSE: 0.1485\n",
            "Epoch [207/1000], Loss: 0.0207, RMSE: 0.1437\n",
            "Epoch [208/1000], Loss: 0.0167, RMSE: 0.1291\n",
            "Epoch [209/1000], Loss: 0.0229, RMSE: 0.1512\n",
            "Epoch [210/1000], Loss: 0.0259, RMSE: 0.1611\n",
            "Epoch [211/1000], Loss: 0.0147, RMSE: 0.1214\n",
            "Epoch [212/1000], Loss: 0.0269, RMSE: 0.1641\n",
            "Epoch [213/1000], Loss: 0.0278, RMSE: 0.1668\n",
            "Epoch [214/1000], Loss: 0.0184, RMSE: 0.1358\n",
            "Epoch [215/1000], Loss: 0.0226, RMSE: 0.1504\n",
            "Epoch [216/1000], Loss: 0.0205, RMSE: 0.1431\n",
            "Epoch [217/1000], Loss: 0.0294, RMSE: 0.1715\n",
            "Epoch [218/1000], Loss: 0.0373, RMSE: 0.1931\n",
            "Epoch [219/1000], Loss: 0.0154, RMSE: 0.1240\n",
            "Epoch [220/1000], Loss: 0.0155, RMSE: 0.1243\n",
            "Epoch [221/1000], Loss: 0.0263, RMSE: 0.1622\n",
            "Epoch [222/1000], Loss: 0.0279, RMSE: 0.1670\n",
            "Epoch [223/1000], Loss: 0.0205, RMSE: 0.1433\n",
            "Epoch [224/1000], Loss: 0.0278, RMSE: 0.1667\n",
            "Epoch [225/1000], Loss: 0.0263, RMSE: 0.1621\n",
            "Epoch [226/1000], Loss: 0.0314, RMSE: 0.1773\n",
            "Epoch [227/1000], Loss: 0.0367, RMSE: 0.1915\n",
            "Epoch [228/1000], Loss: 0.0276, RMSE: 0.1662\n",
            "Epoch [229/1000], Loss: 0.0094, RMSE: 0.0969\n",
            "Epoch [230/1000], Loss: 0.0374, RMSE: 0.1933\n",
            "Epoch [231/1000], Loss: 0.0262, RMSE: 0.1620\n",
            "Epoch [232/1000], Loss: 0.0199, RMSE: 0.1411\n",
            "Epoch [233/1000], Loss: 0.0272, RMSE: 0.1650\n",
            "Epoch [234/1000], Loss: 0.0133, RMSE: 0.1152\n",
            "Epoch [235/1000], Loss: 0.0347, RMSE: 0.1863\n",
            "Epoch [236/1000], Loss: 0.0159, RMSE: 0.1261\n",
            "Epoch [237/1000], Loss: 0.0148, RMSE: 0.1215\n",
            "Epoch [238/1000], Loss: 0.0178, RMSE: 0.1333\n",
            "Epoch [239/1000], Loss: 0.0258, RMSE: 0.1606\n",
            "Epoch [240/1000], Loss: 0.0298, RMSE: 0.1726\n",
            "Epoch [241/1000], Loss: 0.0116, RMSE: 0.1075\n",
            "Epoch [242/1000], Loss: 0.0143, RMSE: 0.1195\n",
            "Epoch [243/1000], Loss: 0.0379, RMSE: 0.1947\n",
            "Epoch [244/1000], Loss: 0.0153, RMSE: 0.1237\n",
            "Epoch [245/1000], Loss: 0.0307, RMSE: 0.1751\n",
            "Epoch [246/1000], Loss: 0.0074, RMSE: 0.0860\n",
            "Epoch [247/1000], Loss: 0.0170, RMSE: 0.1304\n",
            "Epoch [248/1000], Loss: 0.0275, RMSE: 0.1659\n",
            "Epoch [249/1000], Loss: 0.0191, RMSE: 0.1383\n",
            "Epoch [250/1000], Loss: 0.0181, RMSE: 0.1344\n",
            "Epoch [251/1000], Loss: 0.0239, RMSE: 0.1546\n",
            "Epoch [252/1000], Loss: 0.0176, RMSE: 0.1326\n",
            "Epoch [253/1000], Loss: 0.0223, RMSE: 0.1494\n",
            "Epoch [254/1000], Loss: 0.0190, RMSE: 0.1380\n",
            "Epoch [255/1000], Loss: 0.0090, RMSE: 0.0947\n",
            "Epoch [256/1000], Loss: 0.0169, RMSE: 0.1302\n",
            "Epoch [257/1000], Loss: 0.0226, RMSE: 0.1502\n",
            "Epoch [258/1000], Loss: 0.0117, RMSE: 0.1080\n",
            "Epoch [259/1000], Loss: 0.0086, RMSE: 0.0928\n",
            "Epoch [260/1000], Loss: 0.0124, RMSE: 0.1113\n",
            "Epoch [261/1000], Loss: 0.0215, RMSE: 0.1466\n",
            "Epoch [262/1000], Loss: 0.0158, RMSE: 0.1258\n",
            "Epoch [263/1000], Loss: 0.0105, RMSE: 0.1023\n",
            "Epoch [264/1000], Loss: 0.0110, RMSE: 0.1051\n",
            "Epoch [265/1000], Loss: 0.0207, RMSE: 0.1438\n",
            "Epoch [266/1000], Loss: 0.0212, RMSE: 0.1457\n",
            "Epoch [267/1000], Loss: 0.0120, RMSE: 0.1097\n",
            "Epoch [268/1000], Loss: 0.0157, RMSE: 0.1253\n",
            "Epoch [269/1000], Loss: 0.0080, RMSE: 0.0894\n",
            "Epoch [270/1000], Loss: 0.0181, RMSE: 0.1347\n",
            "Epoch [271/1000], Loss: 0.0147, RMSE: 0.1211\n",
            "Epoch [272/1000], Loss: 0.0250, RMSE: 0.1582\n",
            "Epoch [273/1000], Loss: 0.0293, RMSE: 0.1711\n",
            "Epoch [274/1000], Loss: 0.0171, RMSE: 0.1307\n",
            "Epoch [275/1000], Loss: 0.0254, RMSE: 0.1595\n",
            "Epoch [276/1000], Loss: 0.0121, RMSE: 0.1100\n",
            "Epoch [277/1000], Loss: 0.0159, RMSE: 0.1260\n",
            "Epoch [278/1000], Loss: 0.0140, RMSE: 0.1182\n",
            "Epoch [279/1000], Loss: 0.0163, RMSE: 0.1276\n",
            "Epoch [280/1000], Loss: 0.0089, RMSE: 0.0943\n",
            "Epoch [281/1000], Loss: 0.0220, RMSE: 0.1484\n",
            "Epoch [282/1000], Loss: 0.0147, RMSE: 0.1212\n",
            "Epoch [283/1000], Loss: 0.0275, RMSE: 0.1658\n",
            "Epoch [284/1000], Loss: 0.0166, RMSE: 0.1287\n",
            "Epoch [285/1000], Loss: 0.0245, RMSE: 0.1566\n",
            "Epoch [286/1000], Loss: 0.0220, RMSE: 0.1484\n",
            "Epoch [287/1000], Loss: 0.0215, RMSE: 0.1466\n",
            "Epoch [288/1000], Loss: 0.0204, RMSE: 0.1429\n",
            "Epoch [289/1000], Loss: 0.0242, RMSE: 0.1556\n",
            "Epoch [290/1000], Loss: 0.0149, RMSE: 0.1220\n",
            "Epoch [291/1000], Loss: 0.0277, RMSE: 0.1665\n",
            "Epoch [292/1000], Loss: 0.0296, RMSE: 0.1722\n",
            "Epoch [293/1000], Loss: 0.0173, RMSE: 0.1315\n",
            "Epoch [294/1000], Loss: 0.0245, RMSE: 0.1567\n",
            "Epoch [295/1000], Loss: 0.0156, RMSE: 0.1248\n",
            "Epoch [296/1000], Loss: 0.0235, RMSE: 0.1534\n",
            "Epoch [297/1000], Loss: 0.0098, RMSE: 0.0992\n",
            "Epoch [298/1000], Loss: 0.0144, RMSE: 0.1199\n",
            "Epoch [299/1000], Loss: 0.0258, RMSE: 0.1605\n",
            "Epoch [300/1000], Loss: 0.0129, RMSE: 0.1138\n",
            "Epoch [301/1000], Loss: 0.0276, RMSE: 0.1662\n",
            "Epoch [302/1000], Loss: 0.0278, RMSE: 0.1667\n",
            "Epoch [303/1000], Loss: 0.0146, RMSE: 0.1208\n",
            "Epoch [304/1000], Loss: 0.0157, RMSE: 0.1254\n",
            "Epoch [305/1000], Loss: 0.0125, RMSE: 0.1117\n",
            "Epoch [306/1000], Loss: 0.0116, RMSE: 0.1078\n",
            "Epoch [307/1000], Loss: 0.0163, RMSE: 0.1277\n",
            "Epoch [308/1000], Loss: 0.0152, RMSE: 0.1232\n",
            "Epoch [309/1000], Loss: 0.0089, RMSE: 0.0942\n",
            "Epoch [310/1000], Loss: 0.0186, RMSE: 0.1362\n",
            "Epoch [311/1000], Loss: 0.0168, RMSE: 0.1296\n",
            "Epoch [312/1000], Loss: 0.0268, RMSE: 0.1636\n",
            "Epoch [313/1000], Loss: 0.0169, RMSE: 0.1299\n",
            "Epoch [314/1000], Loss: 0.0131, RMSE: 0.1143\n",
            "Epoch [315/1000], Loss: 0.0214, RMSE: 0.1462\n",
            "Epoch [316/1000], Loss: 0.0248, RMSE: 0.1576\n",
            "Epoch [317/1000], Loss: 0.0218, RMSE: 0.1476\n",
            "Epoch [318/1000], Loss: 0.0166, RMSE: 0.1289\n",
            "Epoch [319/1000], Loss: 0.0314, RMSE: 0.1772\n",
            "Epoch [320/1000], Loss: 0.0196, RMSE: 0.1400\n",
            "Epoch [321/1000], Loss: 0.0271, RMSE: 0.1645\n",
            "Epoch [322/1000], Loss: 0.0283, RMSE: 0.1681\n",
            "Epoch [323/1000], Loss: 0.0182, RMSE: 0.1351\n",
            "Epoch [324/1000], Loss: 0.0132, RMSE: 0.1148\n",
            "Epoch [325/1000], Loss: 0.0198, RMSE: 0.1408\n",
            "Epoch [326/1000], Loss: 0.0108, RMSE: 0.1040\n",
            "Epoch [327/1000], Loss: 0.0104, RMSE: 0.1018\n",
            "Epoch [328/1000], Loss: 0.0142, RMSE: 0.1191\n",
            "Epoch [329/1000], Loss: 0.0102, RMSE: 0.1012\n",
            "Epoch [330/1000], Loss: 0.0109, RMSE: 0.1045\n",
            "Epoch [331/1000], Loss: 0.0168, RMSE: 0.1296\n",
            "Epoch [332/1000], Loss: 0.0149, RMSE: 0.1221\n",
            "Epoch [333/1000], Loss: 0.0210, RMSE: 0.1451\n",
            "Epoch [334/1000], Loss: 0.0182, RMSE: 0.1348\n",
            "Epoch [335/1000], Loss: 0.0328, RMSE: 0.1811\n",
            "Epoch [336/1000], Loss: 0.0474, RMSE: 0.2177\n",
            "Epoch [337/1000], Loss: 0.0158, RMSE: 0.1256\n",
            "Epoch [338/1000], Loss: 0.0187, RMSE: 0.1368\n",
            "Epoch [339/1000], Loss: 0.0171, RMSE: 0.1308\n",
            "Epoch [340/1000], Loss: 0.0094, RMSE: 0.0970\n",
            "Epoch [341/1000], Loss: 0.0186, RMSE: 0.1363\n",
            "Epoch [342/1000], Loss: 0.0195, RMSE: 0.1397\n",
            "Epoch [343/1000], Loss: 0.0155, RMSE: 0.1245\n",
            "Epoch [344/1000], Loss: 0.0161, RMSE: 0.1267\n",
            "Epoch [345/1000], Loss: 0.0204, RMSE: 0.1429\n",
            "Epoch [346/1000], Loss: 0.0229, RMSE: 0.1512\n",
            "Epoch [347/1000], Loss: 0.0167, RMSE: 0.1292\n",
            "Epoch [348/1000], Loss: 0.0136, RMSE: 0.1167\n",
            "Epoch [349/1000], Loss: 0.0158, RMSE: 0.1258\n",
            "Epoch [350/1000], Loss: 0.0277, RMSE: 0.1664\n",
            "Epoch [351/1000], Loss: 0.0269, RMSE: 0.1639\n",
            "Epoch [352/1000], Loss: 0.0076, RMSE: 0.0869\n",
            "Epoch [353/1000], Loss: 0.0300, RMSE: 0.1731\n",
            "Epoch [354/1000], Loss: 0.0253, RMSE: 0.1591\n",
            "Epoch [355/1000], Loss: 0.0261, RMSE: 0.1616\n",
            "Epoch [356/1000], Loss: 0.0257, RMSE: 0.1605\n",
            "Epoch [357/1000], Loss: 0.0111, RMSE: 0.1054\n",
            "Epoch [358/1000], Loss: 0.0171, RMSE: 0.1307\n",
            "Epoch [359/1000], Loss: 0.0284, RMSE: 0.1684\n",
            "Epoch [360/1000], Loss: 0.0117, RMSE: 0.1084\n",
            "Epoch [361/1000], Loss: 0.0160, RMSE: 0.1266\n",
            "Epoch [362/1000], Loss: 0.0184, RMSE: 0.1356\n",
            "Epoch [363/1000], Loss: 0.0157, RMSE: 0.1252\n",
            "Epoch [364/1000], Loss: 0.0217, RMSE: 0.1474\n",
            "Epoch [365/1000], Loss: 0.0157, RMSE: 0.1251\n",
            "Epoch [366/1000], Loss: 0.0103, RMSE: 0.1017\n",
            "Epoch [367/1000], Loss: 0.0127, RMSE: 0.1125\n",
            "Epoch [368/1000], Loss: 0.0056, RMSE: 0.0747\n",
            "Epoch [369/1000], Loss: 0.0105, RMSE: 0.1026\n",
            "Epoch [370/1000], Loss: 0.0194, RMSE: 0.1393\n",
            "Epoch [371/1000], Loss: 0.0111, RMSE: 0.1054\n",
            "Epoch [372/1000], Loss: 0.0231, RMSE: 0.1521\n",
            "Epoch [373/1000], Loss: 0.0173, RMSE: 0.1314\n",
            "Epoch [374/1000], Loss: 0.0193, RMSE: 0.1391\n",
            "Epoch [375/1000], Loss: 0.0191, RMSE: 0.1381\n",
            "Epoch [376/1000], Loss: 0.0141, RMSE: 0.1188\n",
            "Epoch [377/1000], Loss: 0.0048, RMSE: 0.0694\n",
            "Epoch [378/1000], Loss: 0.0097, RMSE: 0.0983\n",
            "Epoch [379/1000], Loss: 0.0110, RMSE: 0.1050\n",
            "Epoch [380/1000], Loss: 0.0098, RMSE: 0.0990\n",
            "Epoch [381/1000], Loss: 0.0157, RMSE: 0.1252\n",
            "Epoch [382/1000], Loss: 0.0090, RMSE: 0.0950\n",
            "Epoch [383/1000], Loss: 0.0089, RMSE: 0.0945\n",
            "Epoch [384/1000], Loss: 0.0155, RMSE: 0.1243\n",
            "Epoch [385/1000], Loss: 0.0148, RMSE: 0.1215\n",
            "Epoch [386/1000], Loss: 0.0172, RMSE: 0.1313\n",
            "Epoch [387/1000], Loss: 0.0142, RMSE: 0.1193\n",
            "Epoch [388/1000], Loss: 0.0167, RMSE: 0.1293\n",
            "Epoch [389/1000], Loss: 0.0112, RMSE: 0.1060\n",
            "Epoch [390/1000], Loss: 0.0106, RMSE: 0.1031\n",
            "Epoch [391/1000], Loss: 0.0087, RMSE: 0.0933\n",
            "Epoch [392/1000], Loss: 0.0110, RMSE: 0.1051\n",
            "Epoch [393/1000], Loss: 0.0085, RMSE: 0.0923\n",
            "Epoch [394/1000], Loss: 0.0068, RMSE: 0.0822\n",
            "Epoch [395/1000], Loss: 0.0133, RMSE: 0.1154\n",
            "Epoch [396/1000], Loss: 0.0111, RMSE: 0.1056\n",
            "Epoch [397/1000], Loss: 0.0127, RMSE: 0.1129\n",
            "Epoch [398/1000], Loss: 0.0249, RMSE: 0.1576\n",
            "Epoch [399/1000], Loss: 0.0122, RMSE: 0.1106\n",
            "Epoch [400/1000], Loss: 0.0091, RMSE: 0.0952\n",
            "Epoch [401/1000], Loss: 0.0337, RMSE: 0.1835\n",
            "Epoch [402/1000], Loss: 0.0136, RMSE: 0.1166\n",
            "Epoch [403/1000], Loss: 0.0258, RMSE: 0.1607\n",
            "Epoch [404/1000], Loss: 0.0119, RMSE: 0.1092\n",
            "Epoch [405/1000], Loss: 0.0230, RMSE: 0.1518\n",
            "Epoch [406/1000], Loss: 0.0101, RMSE: 0.1005\n",
            "Epoch [407/1000], Loss: 0.0342, RMSE: 0.1849\n",
            "Epoch [408/1000], Loss: 0.0162, RMSE: 0.1273\n",
            "Epoch [409/1000], Loss: 0.0253, RMSE: 0.1589\n",
            "Epoch [410/1000], Loss: 0.0227, RMSE: 0.1505\n",
            "Epoch [411/1000], Loss: 0.0346, RMSE: 0.1859\n",
            "Epoch [412/1000], Loss: 0.0267, RMSE: 0.1635\n",
            "Epoch [413/1000], Loss: 0.0139, RMSE: 0.1177\n",
            "Epoch [414/1000], Loss: 0.0131, RMSE: 0.1143\n",
            "Epoch [415/1000], Loss: 0.0119, RMSE: 0.1090\n",
            "Epoch [416/1000], Loss: 0.0118, RMSE: 0.1088\n",
            "Epoch [417/1000], Loss: 0.0202, RMSE: 0.1420\n",
            "Epoch [418/1000], Loss: 0.0157, RMSE: 0.1253\n",
            "Epoch [419/1000], Loss: 0.0052, RMSE: 0.0719\n",
            "Epoch [420/1000], Loss: 0.0396, RMSE: 0.1990\n",
            "Epoch [421/1000], Loss: 0.0159, RMSE: 0.1262\n",
            "Epoch [422/1000], Loss: 0.0142, RMSE: 0.1193\n",
            "Epoch [423/1000], Loss: 0.0083, RMSE: 0.0912\n",
            "Epoch [424/1000], Loss: 0.0287, RMSE: 0.1696\n",
            "Epoch [425/1000], Loss: 0.0243, RMSE: 0.1558\n",
            "Epoch [426/1000], Loss: 0.0149, RMSE: 0.1220\n",
            "Epoch [427/1000], Loss: 0.0158, RMSE: 0.1259\n",
            "Epoch [428/1000], Loss: 0.0103, RMSE: 0.1015\n",
            "Epoch [429/1000], Loss: 0.0145, RMSE: 0.1203\n",
            "Epoch [430/1000], Loss: 0.0193, RMSE: 0.1388\n",
            "Epoch [431/1000], Loss: 0.0359, RMSE: 0.1894\n",
            "Epoch [432/1000], Loss: 0.0120, RMSE: 0.1095\n",
            "Epoch [433/1000], Loss: 0.0070, RMSE: 0.0835\n",
            "Epoch [434/1000], Loss: 0.0166, RMSE: 0.1289\n",
            "Epoch [435/1000], Loss: 0.0192, RMSE: 0.1386\n",
            "Epoch [436/1000], Loss: 0.0208, RMSE: 0.1441\n",
            "Epoch [437/1000], Loss: 0.0163, RMSE: 0.1276\n",
            "Epoch [438/1000], Loss: 0.0143, RMSE: 0.1197\n",
            "Epoch [439/1000], Loss: 0.0137, RMSE: 0.1171\n",
            "Epoch [440/1000], Loss: 0.0144, RMSE: 0.1199\n",
            "Epoch [441/1000], Loss: 0.0225, RMSE: 0.1499\n",
            "Epoch [442/1000], Loss: 0.0099, RMSE: 0.0993\n",
            "Epoch [443/1000], Loss: 0.0167, RMSE: 0.1291\n",
            "Epoch [444/1000], Loss: 0.0081, RMSE: 0.0899\n",
            "Epoch [445/1000], Loss: 0.0154, RMSE: 0.1239\n",
            "Epoch [446/1000], Loss: 0.0063, RMSE: 0.0791\n",
            "Epoch [447/1000], Loss: 0.0201, RMSE: 0.1419\n",
            "Epoch [448/1000], Loss: 0.0148, RMSE: 0.1218\n",
            "Epoch [449/1000], Loss: 0.0123, RMSE: 0.1109\n",
            "Epoch [450/1000], Loss: 0.0092, RMSE: 0.0960\n",
            "Epoch [451/1000], Loss: 0.0155, RMSE: 0.1245\n",
            "Epoch [452/1000], Loss: 0.0155, RMSE: 0.1247\n",
            "Epoch [453/1000], Loss: 0.0165, RMSE: 0.1285\n",
            "Epoch [454/1000], Loss: 0.0388, RMSE: 0.1970\n",
            "Epoch [455/1000], Loss: 0.0133, RMSE: 0.1154\n",
            "Epoch [456/1000], Loss: 0.0115, RMSE: 0.1074\n",
            "Epoch [457/1000], Loss: 0.0075, RMSE: 0.0868\n",
            "Epoch [458/1000], Loss: 0.0129, RMSE: 0.1138\n",
            "Epoch [459/1000], Loss: 0.0096, RMSE: 0.0980\n",
            "Epoch [460/1000], Loss: 0.0140, RMSE: 0.1184\n",
            "Epoch [461/1000], Loss: 0.0078, RMSE: 0.0885\n",
            "Epoch [462/1000], Loss: 0.0161, RMSE: 0.1269\n",
            "Epoch [463/1000], Loss: 0.0139, RMSE: 0.1180\n",
            "Epoch [464/1000], Loss: 0.0138, RMSE: 0.1175\n",
            "Epoch [465/1000], Loss: 0.0099, RMSE: 0.0994\n",
            "Epoch [466/1000], Loss: 0.0132, RMSE: 0.1148\n",
            "Epoch [467/1000], Loss: 0.0195, RMSE: 0.1397\n",
            "Epoch [468/1000], Loss: 0.0120, RMSE: 0.1097\n",
            "Epoch [469/1000], Loss: 0.0138, RMSE: 0.1175\n",
            "Epoch [470/1000], Loss: 0.0139, RMSE: 0.1179\n",
            "Epoch [471/1000], Loss: 0.0260, RMSE: 0.1614\n",
            "Epoch [472/1000], Loss: 0.0142, RMSE: 0.1191\n",
            "Epoch [473/1000], Loss: 0.0106, RMSE: 0.1028\n",
            "Epoch [474/1000], Loss: 0.0055, RMSE: 0.0740\n",
            "Epoch [475/1000], Loss: 0.0143, RMSE: 0.1195\n",
            "Epoch [476/1000], Loss: 0.0190, RMSE: 0.1379\n",
            "Epoch [477/1000], Loss: 0.0035, RMSE: 0.0589\n",
            "Epoch [478/1000], Loss: 0.0092, RMSE: 0.0960\n",
            "Epoch [479/1000], Loss: 0.0118, RMSE: 0.1088\n",
            "Epoch [480/1000], Loss: 0.0081, RMSE: 0.0898\n",
            "Epoch [481/1000], Loss: 0.0174, RMSE: 0.1321\n",
            "Epoch [482/1000], Loss: 0.0135, RMSE: 0.1162\n",
            "Epoch [483/1000], Loss: 0.0273, RMSE: 0.1652\n",
            "Epoch [484/1000], Loss: 0.0157, RMSE: 0.1254\n",
            "Epoch [485/1000], Loss: 0.0141, RMSE: 0.1188\n",
            "Epoch [486/1000], Loss: 0.0095, RMSE: 0.0974\n",
            "Epoch [487/1000], Loss: 0.0120, RMSE: 0.1096\n",
            "Epoch [488/1000], Loss: 0.0146, RMSE: 0.1209\n",
            "Epoch [489/1000], Loss: 0.0143, RMSE: 0.1195\n",
            "Epoch [490/1000], Loss: 0.0081, RMSE: 0.0903\n",
            "Epoch [491/1000], Loss: 0.0105, RMSE: 0.1025\n",
            "Epoch [492/1000], Loss: 0.0126, RMSE: 0.1124\n",
            "Epoch [493/1000], Loss: 0.0114, RMSE: 0.1066\n",
            "Epoch [494/1000], Loss: 0.0104, RMSE: 0.1020\n",
            "Epoch [495/1000], Loss: 0.0094, RMSE: 0.0972\n",
            "Epoch [496/1000], Loss: 0.0188, RMSE: 0.1372\n",
            "Epoch [497/1000], Loss: 0.0098, RMSE: 0.0990\n",
            "Epoch [498/1000], Loss: 0.0125, RMSE: 0.1117\n",
            "Epoch [499/1000], Loss: 0.0201, RMSE: 0.1417\n",
            "Epoch [500/1000], Loss: 0.0121, RMSE: 0.1099\n",
            "Epoch [501/1000], Loss: 0.0168, RMSE: 0.1296\n",
            "Epoch [502/1000], Loss: 0.0109, RMSE: 0.1044\n",
            "Epoch [503/1000], Loss: 0.0129, RMSE: 0.1137\n",
            "Epoch [504/1000], Loss: 0.0222, RMSE: 0.1490\n",
            "Epoch [505/1000], Loss: 0.0174, RMSE: 0.1318\n",
            "Epoch [506/1000], Loss: 0.0097, RMSE: 0.0987\n",
            "Epoch [507/1000], Loss: 0.0107, RMSE: 0.1034\n",
            "Epoch [508/1000], Loss: 0.0076, RMSE: 0.0872\n",
            "Epoch [509/1000], Loss: 0.0085, RMSE: 0.0921\n",
            "Epoch [510/1000], Loss: 0.0108, RMSE: 0.1038\n",
            "Epoch [511/1000], Loss: 0.0180, RMSE: 0.1341\n",
            "Epoch [512/1000], Loss: 0.0087, RMSE: 0.0933\n",
            "Epoch [513/1000], Loss: 0.0174, RMSE: 0.1320\n",
            "Epoch [514/1000], Loss: 0.0131, RMSE: 0.1143\n",
            "Epoch [515/1000], Loss: 0.0177, RMSE: 0.1329\n",
            "Epoch [516/1000], Loss: 0.0102, RMSE: 0.1011\n",
            "Epoch [517/1000], Loss: 0.0128, RMSE: 0.1130\n",
            "Epoch [518/1000], Loss: 0.0231, RMSE: 0.1521\n",
            "Epoch [519/1000], Loss: 0.0368, RMSE: 0.1920\n",
            "Epoch [520/1000], Loss: 0.0176, RMSE: 0.1326\n",
            "Epoch [521/1000], Loss: 0.0145, RMSE: 0.1203\n",
            "Epoch [522/1000], Loss: 0.0274, RMSE: 0.1654\n",
            "Epoch [523/1000], Loss: 0.0394, RMSE: 0.1984\n",
            "Epoch [524/1000], Loss: 0.0282, RMSE: 0.1679\n",
            "Epoch [525/1000], Loss: 0.0056, RMSE: 0.0745\n",
            "Epoch [526/1000], Loss: 0.0094, RMSE: 0.0969\n",
            "Epoch [527/1000], Loss: 0.0305, RMSE: 0.1746\n",
            "Epoch [528/1000], Loss: 0.0363, RMSE: 0.1904\n",
            "Epoch [529/1000], Loss: 0.0070, RMSE: 0.0838\n",
            "Epoch [530/1000], Loss: 0.0087, RMSE: 0.0933\n",
            "Epoch [531/1000], Loss: 0.0166, RMSE: 0.1289\n",
            "Epoch [532/1000], Loss: 0.0382, RMSE: 0.1954\n",
            "Epoch [533/1000], Loss: 0.0232, RMSE: 0.1524\n",
            "Epoch [534/1000], Loss: 0.0059, RMSE: 0.0766\n",
            "Epoch [535/1000], Loss: 0.0071, RMSE: 0.0845\n",
            "Epoch [536/1000], Loss: 0.0152, RMSE: 0.1233\n",
            "Epoch [537/1000], Loss: 0.0239, RMSE: 0.1544\n",
            "Epoch [538/1000], Loss: 0.0252, RMSE: 0.1587\n",
            "Epoch [539/1000], Loss: 0.0130, RMSE: 0.1139\n",
            "Epoch [540/1000], Loss: 0.0117, RMSE: 0.1082\n",
            "Epoch [541/1000], Loss: 0.0173, RMSE: 0.1314\n",
            "Epoch [542/1000], Loss: 0.0221, RMSE: 0.1486\n",
            "Epoch [543/1000], Loss: 0.0165, RMSE: 0.1283\n",
            "Epoch [544/1000], Loss: 0.0093, RMSE: 0.0965\n",
            "Epoch [545/1000], Loss: 0.0100, RMSE: 0.1002\n",
            "Epoch [546/1000], Loss: 0.0113, RMSE: 0.1065\n",
            "Epoch [547/1000], Loss: 0.0103, RMSE: 0.1012\n",
            "Epoch [548/1000], Loss: 0.0110, RMSE: 0.1047\n",
            "Epoch [549/1000], Loss: 0.0116, RMSE: 0.1075\n",
            "Epoch [550/1000], Loss: 0.0138, RMSE: 0.1176\n",
            "Epoch [551/1000], Loss: 0.0040, RMSE: 0.0633\n",
            "Epoch [552/1000], Loss: 0.0132, RMSE: 0.1150\n",
            "Epoch [553/1000], Loss: 0.0282, RMSE: 0.1681\n",
            "Epoch [554/1000], Loss: 0.0082, RMSE: 0.0904\n",
            "Epoch [555/1000], Loss: 0.0135, RMSE: 0.1163\n",
            "Epoch [556/1000], Loss: 0.0160, RMSE: 0.1266\n",
            "Epoch [557/1000], Loss: 0.0100, RMSE: 0.1001\n",
            "Epoch [558/1000], Loss: 0.0071, RMSE: 0.0840\n",
            "Epoch [559/1000], Loss: 0.0146, RMSE: 0.1207\n",
            "Epoch [560/1000], Loss: 0.0039, RMSE: 0.0626\n",
            "Epoch [561/1000], Loss: 0.0247, RMSE: 0.1571\n",
            "Epoch [562/1000], Loss: 0.0069, RMSE: 0.0829\n",
            "Epoch [563/1000], Loss: 0.0137, RMSE: 0.1171\n",
            "Epoch [564/1000], Loss: 0.0084, RMSE: 0.0919\n",
            "Epoch [565/1000], Loss: 0.0054, RMSE: 0.0734\n",
            "Epoch [566/1000], Loss: 0.0176, RMSE: 0.1328\n",
            "Epoch [567/1000], Loss: 0.0100, RMSE: 0.1002\n",
            "Epoch [568/1000], Loss: 0.0087, RMSE: 0.0932\n",
            "Epoch [569/1000], Loss: 0.0091, RMSE: 0.0952\n",
            "Epoch [570/1000], Loss: 0.0130, RMSE: 0.1142\n",
            "Epoch [571/1000], Loss: 0.0145, RMSE: 0.1206\n",
            "Epoch [572/1000], Loss: 0.0086, RMSE: 0.0926\n",
            "Epoch [573/1000], Loss: 0.0115, RMSE: 0.1073\n",
            "Epoch [574/1000], Loss: 0.0038, RMSE: 0.0616\n",
            "Epoch [575/1000], Loss: 0.0079, RMSE: 0.0887\n",
            "Epoch [576/1000], Loss: 0.0157, RMSE: 0.1251\n",
            "Epoch [577/1000], Loss: 0.0102, RMSE: 0.1008\n",
            "Epoch [578/1000], Loss: 0.0113, RMSE: 0.1063\n",
            "Epoch [579/1000], Loss: 0.0124, RMSE: 0.1115\n",
            "Epoch [580/1000], Loss: 0.0151, RMSE: 0.1227\n",
            "Epoch [581/1000], Loss: 0.0102, RMSE: 0.1008\n",
            "Epoch [582/1000], Loss: 0.0186, RMSE: 0.1365\n",
            "Epoch [583/1000], Loss: 0.0089, RMSE: 0.0943\n",
            "Epoch [584/1000], Loss: 0.0111, RMSE: 0.1054\n",
            "Epoch [585/1000], Loss: 0.0110, RMSE: 0.1048\n",
            "Epoch [586/1000], Loss: 0.0154, RMSE: 0.1240\n",
            "Epoch [587/1000], Loss: 0.0136, RMSE: 0.1168\n",
            "Epoch [588/1000], Loss: 0.0097, RMSE: 0.0987\n",
            "Epoch [589/1000], Loss: 0.0094, RMSE: 0.0970\n",
            "Epoch [590/1000], Loss: 0.0101, RMSE: 0.1007\n",
            "Epoch [591/1000], Loss: 0.0069, RMSE: 0.0831\n",
            "Epoch [592/1000], Loss: 0.0072, RMSE: 0.0849\n",
            "Epoch [593/1000], Loss: 0.0162, RMSE: 0.1271\n",
            "Epoch [594/1000], Loss: 0.0104, RMSE: 0.1022\n",
            "Epoch [595/1000], Loss: 0.0124, RMSE: 0.1112\n",
            "Epoch [596/1000], Loss: 0.0108, RMSE: 0.1040\n",
            "Epoch [597/1000], Loss: 0.0127, RMSE: 0.1127\n",
            "Epoch [598/1000], Loss: 0.0115, RMSE: 0.1072\n",
            "Epoch [599/1000], Loss: 0.0075, RMSE: 0.0863\n",
            "Epoch [600/1000], Loss: 0.0081, RMSE: 0.0899\n",
            "Epoch [601/1000], Loss: 0.0229, RMSE: 0.1513\n",
            "Epoch [602/1000], Loss: 0.0103, RMSE: 0.1015\n",
            "Epoch [603/1000], Loss: 0.0092, RMSE: 0.0961\n",
            "Epoch [604/1000], Loss: 0.0224, RMSE: 0.1496\n",
            "Epoch [605/1000], Loss: 0.0089, RMSE: 0.0944\n",
            "Epoch [606/1000], Loss: 0.0089, RMSE: 0.0944\n",
            "Epoch [607/1000], Loss: 0.0185, RMSE: 0.1360\n",
            "Epoch [608/1000], Loss: 0.0061, RMSE: 0.0783\n",
            "Epoch [609/1000], Loss: 0.0189, RMSE: 0.1375\n",
            "Epoch [610/1000], Loss: 0.0066, RMSE: 0.0814\n",
            "Epoch [611/1000], Loss: 0.0098, RMSE: 0.0991\n",
            "Epoch [612/1000], Loss: 0.0098, RMSE: 0.0992\n",
            "Epoch [613/1000], Loss: 0.0100, RMSE: 0.1001\n",
            "Epoch [614/1000], Loss: 0.0101, RMSE: 0.1004\n",
            "Epoch [615/1000], Loss: 0.0065, RMSE: 0.0804\n",
            "Epoch [616/1000], Loss: 0.0084, RMSE: 0.0916\n",
            "Epoch [617/1000], Loss: 0.0195, RMSE: 0.1396\n",
            "Epoch [618/1000], Loss: 0.0059, RMSE: 0.0770\n",
            "Epoch [619/1000], Loss: 0.0074, RMSE: 0.0862\n",
            "Epoch [620/1000], Loss: 0.0070, RMSE: 0.0840\n",
            "Epoch [621/1000], Loss: 0.0065, RMSE: 0.0805\n",
            "Epoch [622/1000], Loss: 0.0075, RMSE: 0.0867\n",
            "Epoch [623/1000], Loss: 0.0053, RMSE: 0.0726\n",
            "Epoch [624/1000], Loss: 0.0052, RMSE: 0.0720\n",
            "Epoch [625/1000], Loss: 0.0133, RMSE: 0.1155\n",
            "Epoch [626/1000], Loss: 0.0114, RMSE: 0.1067\n",
            "Epoch [627/1000], Loss: 0.0060, RMSE: 0.0773\n",
            "Epoch [628/1000], Loss: 0.0142, RMSE: 0.1190\n",
            "Epoch [629/1000], Loss: 0.0073, RMSE: 0.0852\n",
            "Epoch [630/1000], Loss: 0.0075, RMSE: 0.0866\n",
            "Epoch [631/1000], Loss: 0.0091, RMSE: 0.0956\n",
            "Epoch [632/1000], Loss: 0.0112, RMSE: 0.1058\n",
            "Epoch [633/1000], Loss: 0.0078, RMSE: 0.0885\n",
            "Epoch [634/1000], Loss: 0.0069, RMSE: 0.0832\n",
            "Epoch [635/1000], Loss: 0.0300, RMSE: 0.1731\n",
            "Epoch [636/1000], Loss: 0.0184, RMSE: 0.1357\n",
            "Epoch [637/1000], Loss: 0.0196, RMSE: 0.1398\n",
            "Epoch [638/1000], Loss: 0.0122, RMSE: 0.1103\n",
            "Epoch [639/1000], Loss: 0.0081, RMSE: 0.0902\n",
            "Epoch [640/1000], Loss: 0.0157, RMSE: 0.1254\n",
            "Epoch [641/1000], Loss: 0.0179, RMSE: 0.1337\n",
            "Epoch [642/1000], Loss: 0.0063, RMSE: 0.0797\n",
            "Epoch [643/1000], Loss: 0.0071, RMSE: 0.0843\n",
            "Epoch [644/1000], Loss: 0.0118, RMSE: 0.1085\n",
            "Epoch [645/1000], Loss: 0.0087, RMSE: 0.0932\n",
            "Epoch [646/1000], Loss: 0.0057, RMSE: 0.0753\n",
            "Epoch [647/1000], Loss: 0.0135, RMSE: 0.1162\n",
            "Epoch [648/1000], Loss: 0.0122, RMSE: 0.1105\n",
            "Epoch [649/1000], Loss: 0.0180, RMSE: 0.1341\n",
            "Epoch [650/1000], Loss: 0.0138, RMSE: 0.1176\n",
            "Epoch [651/1000], Loss: 0.0084, RMSE: 0.0918\n",
            "Epoch [652/1000], Loss: 0.0062, RMSE: 0.0790\n",
            "Epoch [653/1000], Loss: 0.0065, RMSE: 0.0805\n",
            "Epoch [654/1000], Loss: 0.0092, RMSE: 0.0959\n",
            "Epoch [655/1000], Loss: 0.0093, RMSE: 0.0964\n",
            "Epoch [656/1000], Loss: 0.0087, RMSE: 0.0931\n",
            "Epoch [657/1000], Loss: 0.0090, RMSE: 0.0947\n",
            "Epoch [658/1000], Loss: 0.0108, RMSE: 0.1040\n",
            "Epoch [659/1000], Loss: 0.0112, RMSE: 0.1058\n",
            "Epoch [660/1000], Loss: 0.0099, RMSE: 0.0994\n",
            "Epoch [661/1000], Loss: 0.0166, RMSE: 0.1286\n",
            "Epoch [662/1000], Loss: 0.0100, RMSE: 0.0998\n",
            "Epoch [663/1000], Loss: 0.0174, RMSE: 0.1321\n",
            "Epoch [664/1000], Loss: 0.0115, RMSE: 0.1072\n",
            "Epoch [665/1000], Loss: 0.0079, RMSE: 0.0890\n",
            "Epoch [666/1000], Loss: 0.0058, RMSE: 0.0761\n",
            "Epoch [667/1000], Loss: 0.0166, RMSE: 0.1289\n",
            "Epoch [668/1000], Loss: 0.0105, RMSE: 0.1024\n",
            "Epoch [669/1000], Loss: 0.0159, RMSE: 0.1260\n",
            "Epoch [670/1000], Loss: 0.0061, RMSE: 0.0778\n",
            "Epoch [671/1000], Loss: 0.0120, RMSE: 0.1096\n",
            "Epoch [672/1000], Loss: 0.0160, RMSE: 0.1265\n",
            "Epoch [673/1000], Loss: 0.0062, RMSE: 0.0790\n",
            "Epoch [674/1000], Loss: 0.0094, RMSE: 0.0971\n",
            "Epoch [675/1000], Loss: 0.0176, RMSE: 0.1328\n",
            "Epoch [676/1000], Loss: 0.0089, RMSE: 0.0941\n",
            "Epoch [677/1000], Loss: 0.0153, RMSE: 0.1237\n",
            "Epoch [678/1000], Loss: 0.0062, RMSE: 0.0785\n",
            "Epoch [679/1000], Loss: 0.0129, RMSE: 0.1135\n",
            "Epoch [680/1000], Loss: 0.0148, RMSE: 0.1218\n",
            "Epoch [681/1000], Loss: 0.0192, RMSE: 0.1384\n",
            "Epoch [682/1000], Loss: 0.0065, RMSE: 0.0804\n",
            "Epoch [683/1000], Loss: 0.0089, RMSE: 0.0942\n",
            "Epoch [684/1000], Loss: 0.0141, RMSE: 0.1189\n",
            "Epoch [685/1000], Loss: 0.0167, RMSE: 0.1293\n",
            "Epoch [686/1000], Loss: 0.0096, RMSE: 0.0978\n",
            "Epoch [687/1000], Loss: 0.0104, RMSE: 0.1021\n",
            "Epoch [688/1000], Loss: 0.0057, RMSE: 0.0757\n",
            "Epoch [689/1000], Loss: 0.0046, RMSE: 0.0681\n",
            "Epoch [690/1000], Loss: 0.0075, RMSE: 0.0867\n",
            "Epoch [691/1000], Loss: 0.0038, RMSE: 0.0615\n",
            "Epoch [692/1000], Loss: 0.0070, RMSE: 0.0839\n",
            "Epoch [693/1000], Loss: 0.0086, RMSE: 0.0927\n",
            "Epoch [694/1000], Loss: 0.0082, RMSE: 0.0904\n",
            "Epoch [695/1000], Loss: 0.0044, RMSE: 0.0666\n",
            "Epoch [696/1000], Loss: 0.0127, RMSE: 0.1128\n",
            "Epoch [697/1000], Loss: 0.0088, RMSE: 0.0939\n",
            "Epoch [698/1000], Loss: 0.0062, RMSE: 0.0787\n",
            "Epoch [699/1000], Loss: 0.0088, RMSE: 0.0938\n",
            "Epoch [700/1000], Loss: 0.0135, RMSE: 0.1163\n",
            "Epoch [701/1000], Loss: 0.0063, RMSE: 0.0794\n",
            "Epoch [702/1000], Loss: 0.0070, RMSE: 0.0834\n",
            "Epoch [703/1000], Loss: 0.0080, RMSE: 0.0893\n",
            "Epoch [704/1000], Loss: 0.0092, RMSE: 0.0961\n",
            "Epoch [705/1000], Loss: 0.0084, RMSE: 0.0915\n",
            "Epoch [706/1000], Loss: 0.0080, RMSE: 0.0894\n",
            "Epoch [707/1000], Loss: 0.0040, RMSE: 0.0629\n",
            "Epoch [708/1000], Loss: 0.0037, RMSE: 0.0609\n",
            "Epoch [709/1000], Loss: 0.0073, RMSE: 0.0857\n",
            "Epoch [710/1000], Loss: 0.0110, RMSE: 0.1048\n",
            "Epoch [711/1000], Loss: 0.0151, RMSE: 0.1228\n",
            "Epoch [712/1000], Loss: 0.0090, RMSE: 0.0950\n",
            "Epoch [713/1000], Loss: 0.0116, RMSE: 0.1075\n",
            "Epoch [714/1000], Loss: 0.0114, RMSE: 0.1066\n",
            "Epoch [715/1000], Loss: 0.0060, RMSE: 0.0775\n",
            "Epoch [716/1000], Loss: 0.0085, RMSE: 0.0920\n",
            "Epoch [717/1000], Loss: 0.0056, RMSE: 0.0745\n",
            "Epoch [718/1000], Loss: 0.0096, RMSE: 0.0980\n",
            "Epoch [719/1000], Loss: 0.0118, RMSE: 0.1086\n",
            "Epoch [720/1000], Loss: 0.0108, RMSE: 0.1041\n",
            "Epoch [721/1000], Loss: 0.0130, RMSE: 0.1139\n",
            "Epoch [722/1000], Loss: 0.0269, RMSE: 0.1639\n",
            "Epoch [723/1000], Loss: 0.0046, RMSE: 0.0679\n",
            "Epoch [724/1000], Loss: 0.0099, RMSE: 0.0996\n",
            "Epoch [725/1000], Loss: 0.0032, RMSE: 0.0569\n",
            "Epoch [726/1000], Loss: 0.0037, RMSE: 0.0605\n",
            "Epoch [727/1000], Loss: 0.0073, RMSE: 0.0856\n",
            "Epoch [728/1000], Loss: 0.0096, RMSE: 0.0982\n",
            "Epoch [729/1000], Loss: 0.0074, RMSE: 0.0860\n",
            "Epoch [730/1000], Loss: 0.0135, RMSE: 0.1164\n",
            "Epoch [731/1000], Loss: 0.0082, RMSE: 0.0908\n",
            "Epoch [732/1000], Loss: 0.0077, RMSE: 0.0876\n",
            "Epoch [733/1000], Loss: 0.0104, RMSE: 0.1022\n",
            "Epoch [734/1000], Loss: 0.0024, RMSE: 0.0491\n",
            "Epoch [735/1000], Loss: 0.0138, RMSE: 0.1174\n",
            "Epoch [736/1000], Loss: 0.0112, RMSE: 0.1059\n",
            "Epoch [737/1000], Loss: 0.0142, RMSE: 0.1191\n",
            "Epoch [738/1000], Loss: 0.0081, RMSE: 0.0900\n",
            "Epoch [739/1000], Loss: 0.0059, RMSE: 0.0770\n",
            "Epoch [740/1000], Loss: 0.0104, RMSE: 0.1020\n",
            "Epoch [741/1000], Loss: 0.0084, RMSE: 0.0917\n",
            "Epoch [742/1000], Loss: 0.0077, RMSE: 0.0878\n",
            "Epoch [743/1000], Loss: 0.0049, RMSE: 0.0700\n",
            "Epoch [744/1000], Loss: 0.0139, RMSE: 0.1181\n",
            "Epoch [745/1000], Loss: 0.0121, RMSE: 0.1102\n",
            "Epoch [746/1000], Loss: 0.0145, RMSE: 0.1206\n",
            "Epoch [747/1000], Loss: 0.0063, RMSE: 0.0792\n",
            "Epoch [748/1000], Loss: 0.0076, RMSE: 0.0873\n",
            "Epoch [749/1000], Loss: 0.0082, RMSE: 0.0903\n",
            "Epoch [750/1000], Loss: 0.0108, RMSE: 0.1040\n",
            "Epoch [751/1000], Loss: 0.0141, RMSE: 0.1187\n",
            "Epoch [752/1000], Loss: 0.0111, RMSE: 0.1055\n",
            "Epoch [753/1000], Loss: 0.0088, RMSE: 0.0939\n",
            "Epoch [754/1000], Loss: 0.0075, RMSE: 0.0867\n",
            "Epoch [755/1000], Loss: 0.0201, RMSE: 0.1419\n",
            "Epoch [756/1000], Loss: 0.0115, RMSE: 0.1074\n",
            "Epoch [757/1000], Loss: 0.0112, RMSE: 0.1056\n",
            "Epoch [758/1000], Loss: 0.0089, RMSE: 0.0946\n",
            "Epoch [759/1000], Loss: 0.0091, RMSE: 0.0956\n",
            "Epoch [760/1000], Loss: 0.0091, RMSE: 0.0953\n",
            "Epoch [761/1000], Loss: 0.0194, RMSE: 0.1394\n",
            "Epoch [762/1000], Loss: 0.0135, RMSE: 0.1162\n",
            "Epoch [763/1000], Loss: 0.0080, RMSE: 0.0895\n",
            "Epoch [764/1000], Loss: 0.0138, RMSE: 0.1175\n",
            "Epoch [765/1000], Loss: 0.0172, RMSE: 0.1310\n",
            "Epoch [766/1000], Loss: 0.0225, RMSE: 0.1499\n",
            "Epoch [767/1000], Loss: 0.0073, RMSE: 0.0855\n",
            "Epoch [768/1000], Loss: 0.0093, RMSE: 0.0966\n",
            "Epoch [769/1000], Loss: 0.0105, RMSE: 0.1026\n",
            "Epoch [770/1000], Loss: 0.0122, RMSE: 0.1106\n",
            "Epoch [771/1000], Loss: 0.0263, RMSE: 0.1623\n",
            "Epoch [772/1000], Loss: 0.0092, RMSE: 0.0960\n",
            "Epoch [773/1000], Loss: 0.0043, RMSE: 0.0657\n",
            "Epoch [774/1000], Loss: 0.0075, RMSE: 0.0864\n",
            "Epoch [775/1000], Loss: 0.0150, RMSE: 0.1226\n",
            "Epoch [776/1000], Loss: 0.0111, RMSE: 0.1054\n",
            "Epoch [777/1000], Loss: 0.0139, RMSE: 0.1178\n",
            "Epoch [778/1000], Loss: 0.0081, RMSE: 0.0900\n",
            "Epoch [779/1000], Loss: 0.0048, RMSE: 0.0692\n",
            "Epoch [780/1000], Loss: 0.0079, RMSE: 0.0888\n",
            "Epoch [781/1000], Loss: 0.0068, RMSE: 0.0826\n",
            "Epoch [782/1000], Loss: 0.0087, RMSE: 0.0933\n",
            "Epoch [783/1000], Loss: 0.0077, RMSE: 0.0877\n",
            "Epoch [784/1000], Loss: 0.0162, RMSE: 0.1274\n",
            "Epoch [785/1000], Loss: 0.0210, RMSE: 0.1449\n",
            "Epoch [786/1000], Loss: 0.0055, RMSE: 0.0739\n",
            "Epoch [787/1000], Loss: 0.0106, RMSE: 0.1028\n",
            "Epoch [788/1000], Loss: 0.0087, RMSE: 0.0935\n",
            "Epoch [789/1000], Loss: 0.0117, RMSE: 0.1082\n",
            "Epoch [790/1000], Loss: 0.0062, RMSE: 0.0787\n",
            "Epoch [791/1000], Loss: 0.0050, RMSE: 0.0706\n",
            "Epoch [792/1000], Loss: 0.0086, RMSE: 0.0927\n",
            "Epoch [793/1000], Loss: 0.0101, RMSE: 0.1004\n",
            "Epoch [794/1000], Loss: 0.0059, RMSE: 0.0769\n",
            "Epoch [795/1000], Loss: 0.0050, RMSE: 0.0705\n",
            "Epoch [796/1000], Loss: 0.0112, RMSE: 0.1057\n",
            "Epoch [797/1000], Loss: 0.0059, RMSE: 0.0766\n",
            "Epoch [798/1000], Loss: 0.0086, RMSE: 0.0928\n",
            "Epoch [799/1000], Loss: 0.0170, RMSE: 0.1303\n",
            "Epoch [800/1000], Loss: 0.0084, RMSE: 0.0916\n",
            "Epoch [801/1000], Loss: 0.0104, RMSE: 0.1020\n",
            "Epoch [802/1000], Loss: 0.0108, RMSE: 0.1039\n",
            "Epoch [803/1000], Loss: 0.0120, RMSE: 0.1093\n",
            "Epoch [804/1000], Loss: 0.0123, RMSE: 0.1109\n",
            "Epoch [805/1000], Loss: 0.0042, RMSE: 0.0645\n",
            "Epoch [806/1000], Loss: 0.0054, RMSE: 0.0733\n",
            "Epoch [807/1000], Loss: 0.0100, RMSE: 0.1000\n",
            "Epoch [808/1000], Loss: 0.0090, RMSE: 0.0947\n",
            "Epoch [809/1000], Loss: 0.0096, RMSE: 0.0978\n",
            "Epoch [810/1000], Loss: 0.0131, RMSE: 0.1143\n",
            "Epoch [811/1000], Loss: 0.0068, RMSE: 0.0825\n",
            "Epoch [812/1000], Loss: 0.0088, RMSE: 0.0939\n",
            "Epoch [813/1000], Loss: 0.0181, RMSE: 0.1344\n",
            "Epoch [814/1000], Loss: 0.0069, RMSE: 0.0830\n",
            "Epoch [815/1000], Loss: 0.0108, RMSE: 0.1038\n",
            "Epoch [816/1000], Loss: 0.0151, RMSE: 0.1229\n",
            "Epoch [817/1000], Loss: 0.0081, RMSE: 0.0903\n",
            "Epoch [818/1000], Loss: 0.0042, RMSE: 0.0646\n",
            "Epoch [819/1000], Loss: 0.0059, RMSE: 0.0766\n",
            "Epoch [820/1000], Loss: 0.0065, RMSE: 0.0808\n",
            "Epoch [821/1000], Loss: 0.0059, RMSE: 0.0771\n",
            "Epoch [822/1000], Loss: 0.0078, RMSE: 0.0884\n",
            "Epoch [823/1000], Loss: 0.0070, RMSE: 0.0839\n",
            "Epoch [824/1000], Loss: 0.0127, RMSE: 0.1126\n",
            "Epoch [825/1000], Loss: 0.0072, RMSE: 0.0849\n",
            "Epoch [826/1000], Loss: 0.0147, RMSE: 0.1214\n",
            "Epoch [827/1000], Loss: 0.0101, RMSE: 0.1004\n",
            "Epoch [828/1000], Loss: 0.0052, RMSE: 0.0724\n",
            "Epoch [829/1000], Loss: 0.0101, RMSE: 0.1006\n",
            "Epoch [830/1000], Loss: 0.0077, RMSE: 0.0878\n",
            "Epoch [831/1000], Loss: 0.0052, RMSE: 0.0723\n",
            "Epoch [832/1000], Loss: 0.0080, RMSE: 0.0893\n",
            "Epoch [833/1000], Loss: 0.0037, RMSE: 0.0608\n",
            "Epoch [834/1000], Loss: 0.0036, RMSE: 0.0600\n",
            "Epoch [835/1000], Loss: 0.0090, RMSE: 0.0948\n",
            "Epoch [836/1000], Loss: 0.0056, RMSE: 0.0745\n",
            "Epoch [837/1000], Loss: 0.0102, RMSE: 0.1008\n",
            "Epoch [838/1000], Loss: 0.0071, RMSE: 0.0843\n",
            "Epoch [839/1000], Loss: 0.0075, RMSE: 0.0865\n",
            "Epoch [840/1000], Loss: 0.0048, RMSE: 0.0693\n",
            "Epoch [841/1000], Loss: 0.0076, RMSE: 0.0872\n",
            "Epoch [842/1000], Loss: 0.0116, RMSE: 0.1075\n",
            "Epoch [843/1000], Loss: 0.0066, RMSE: 0.0815\n",
            "Epoch [844/1000], Loss: 0.0098, RMSE: 0.0990\n",
            "Epoch [845/1000], Loss: 0.0055, RMSE: 0.0740\n",
            "Epoch [846/1000], Loss: 0.0119, RMSE: 0.1091\n",
            "Epoch [847/1000], Loss: 0.0085, RMSE: 0.0921\n",
            "Epoch [848/1000], Loss: 0.0080, RMSE: 0.0896\n",
            "Epoch [849/1000], Loss: 0.0070, RMSE: 0.0835\n",
            "Epoch [850/1000], Loss: 0.0085, RMSE: 0.0923\n",
            "Epoch [851/1000], Loss: 0.0101, RMSE: 0.1004\n",
            "Epoch [852/1000], Loss: 0.0096, RMSE: 0.0982\n",
            "Epoch [853/1000], Loss: 0.0103, RMSE: 0.1015\n",
            "Epoch [854/1000], Loss: 0.0104, RMSE: 0.1021\n",
            "Epoch [855/1000], Loss: 0.0134, RMSE: 0.1158\n",
            "Epoch [856/1000], Loss: 0.0053, RMSE: 0.0730\n",
            "Epoch [857/1000], Loss: 0.0080, RMSE: 0.0894\n",
            "Epoch [858/1000], Loss: 0.0052, RMSE: 0.0719\n",
            "Epoch [859/1000], Loss: 0.0137, RMSE: 0.1170\n",
            "Epoch [860/1000], Loss: 0.0081, RMSE: 0.0900\n",
            "Epoch [861/1000], Loss: 0.0146, RMSE: 0.1208\n",
            "Epoch [862/1000], Loss: 0.0102, RMSE: 0.1012\n",
            "Epoch [863/1000], Loss: 0.0035, RMSE: 0.0596\n",
            "Epoch [864/1000], Loss: 0.0048, RMSE: 0.0692\n",
            "Epoch [865/1000], Loss: 0.0044, RMSE: 0.0664\n",
            "Epoch [866/1000], Loss: 0.0095, RMSE: 0.0975\n",
            "Epoch [867/1000], Loss: 0.0188, RMSE: 0.1371\n",
            "Epoch [868/1000], Loss: 0.0062, RMSE: 0.0786\n",
            "Epoch [869/1000], Loss: 0.0059, RMSE: 0.0766\n",
            "Epoch [870/1000], Loss: 0.0218, RMSE: 0.1478\n",
            "Epoch [871/1000], Loss: 0.0088, RMSE: 0.0939\n",
            "Epoch [872/1000], Loss: 0.0186, RMSE: 0.1363\n",
            "Epoch [873/1000], Loss: 0.0061, RMSE: 0.0782\n",
            "Epoch [874/1000], Loss: 0.0100, RMSE: 0.1001\n",
            "Epoch [875/1000], Loss: 0.0092, RMSE: 0.0961\n",
            "Epoch [876/1000], Loss: 0.0084, RMSE: 0.0916\n",
            "Epoch [877/1000], Loss: 0.0098, RMSE: 0.0992\n",
            "Epoch [878/1000], Loss: 0.0064, RMSE: 0.0802\n",
            "Epoch [879/1000], Loss: 0.0091, RMSE: 0.0954\n",
            "Epoch [880/1000], Loss: 0.0093, RMSE: 0.0963\n",
            "Epoch [881/1000], Loss: 0.0061, RMSE: 0.0780\n",
            "Epoch [882/1000], Loss: 0.0081, RMSE: 0.0903\n",
            "Epoch [883/1000], Loss: 0.0096, RMSE: 0.0982\n",
            "Epoch [884/1000], Loss: 0.0102, RMSE: 0.1008\n",
            "Epoch [885/1000], Loss: 0.0095, RMSE: 0.0973\n",
            "Epoch [886/1000], Loss: 0.0080, RMSE: 0.0894\n",
            "Epoch [887/1000], Loss: 0.0072, RMSE: 0.0846\n",
            "Epoch [888/1000], Loss: 0.0074, RMSE: 0.0863\n",
            "Epoch [889/1000], Loss: 0.0116, RMSE: 0.1078\n",
            "Epoch [890/1000], Loss: 0.0145, RMSE: 0.1206\n",
            "Epoch [891/1000], Loss: 0.0042, RMSE: 0.0646\n",
            "Epoch [892/1000], Loss: 0.0091, RMSE: 0.0955\n",
            "Epoch [893/1000], Loss: 0.0060, RMSE: 0.0773\n",
            "Epoch [894/1000], Loss: 0.0072, RMSE: 0.0850\n",
            "Epoch [895/1000], Loss: 0.0069, RMSE: 0.0829\n",
            "Epoch [896/1000], Loss: 0.0101, RMSE: 0.1004\n",
            "Epoch [897/1000], Loss: 0.0078, RMSE: 0.0885\n",
            "Epoch [898/1000], Loss: 0.0105, RMSE: 0.1024\n",
            "Epoch [899/1000], Loss: 0.0068, RMSE: 0.0826\n",
            "Epoch [900/1000], Loss: 0.0108, RMSE: 0.1039\n",
            "Epoch [901/1000], Loss: 0.0130, RMSE: 0.1139\n",
            "Epoch [902/1000], Loss: 0.0085, RMSE: 0.0920\n",
            "Epoch [903/1000], Loss: 0.0076, RMSE: 0.0871\n",
            "Epoch [904/1000], Loss: 0.0051, RMSE: 0.0714\n",
            "Epoch [905/1000], Loss: 0.0125, RMSE: 0.1119\n",
            "Epoch [906/1000], Loss: 0.0061, RMSE: 0.0780\n",
            "Epoch [907/1000], Loss: 0.0049, RMSE: 0.0698\n",
            "Epoch [908/1000], Loss: 0.0061, RMSE: 0.0779\n",
            "Epoch [909/1000], Loss: 0.0072, RMSE: 0.0851\n",
            "Epoch [910/1000], Loss: 0.0114, RMSE: 0.1067\n",
            "Epoch [911/1000], Loss: 0.0110, RMSE: 0.1050\n",
            "Epoch [912/1000], Loss: 0.0065, RMSE: 0.0809\n",
            "Epoch [913/1000], Loss: 0.0094, RMSE: 0.0969\n",
            "Epoch [914/1000], Loss: 0.0093, RMSE: 0.0967\n",
            "Epoch [915/1000], Loss: 0.0078, RMSE: 0.0883\n",
            "Epoch [916/1000], Loss: 0.0123, RMSE: 0.1110\n",
            "Epoch [917/1000], Loss: 0.0044, RMSE: 0.0664\n",
            "Epoch [918/1000], Loss: 0.0028, RMSE: 0.0532\n",
            "Epoch [919/1000], Loss: 0.0056, RMSE: 0.0745\n",
            "Epoch [920/1000], Loss: 0.0036, RMSE: 0.0602\n",
            "Epoch [921/1000], Loss: 0.0106, RMSE: 0.1029\n",
            "Epoch [922/1000], Loss: 0.0088, RMSE: 0.0939\n",
            "Epoch [923/1000], Loss: 0.0060, RMSE: 0.0776\n",
            "Epoch [924/1000], Loss: 0.0173, RMSE: 0.1314\n",
            "Epoch [925/1000], Loss: 0.0045, RMSE: 0.0667\n",
            "Epoch [926/1000], Loss: 0.0136, RMSE: 0.1165\n",
            "Epoch [927/1000], Loss: 0.0124, RMSE: 0.1113\n",
            "Epoch [928/1000], Loss: 0.0080, RMSE: 0.0896\n",
            "Epoch [929/1000], Loss: 0.0085, RMSE: 0.0922\n",
            "Epoch [930/1000], Loss: 0.0020, RMSE: 0.0449\n",
            "Epoch [931/1000], Loss: 0.0057, RMSE: 0.0757\n",
            "Epoch [932/1000], Loss: 0.0043, RMSE: 0.0659\n",
            "Epoch [933/1000], Loss: 0.0063, RMSE: 0.0793\n",
            "Epoch [934/1000], Loss: 0.0053, RMSE: 0.0729\n",
            "Epoch [935/1000], Loss: 0.0046, RMSE: 0.0676\n",
            "Epoch [936/1000], Loss: 0.0070, RMSE: 0.0834\n",
            "Epoch [937/1000], Loss: 0.0070, RMSE: 0.0835\n",
            "Epoch [938/1000], Loss: 0.0047, RMSE: 0.0684\n",
            "Epoch [939/1000], Loss: 0.0032, RMSE: 0.0563\n",
            "Epoch [940/1000], Loss: 0.0056, RMSE: 0.0749\n",
            "Epoch [941/1000], Loss: 0.0080, RMSE: 0.0896\n",
            "Epoch [942/1000], Loss: 0.0067, RMSE: 0.0821\n",
            "Epoch [943/1000], Loss: 0.0084, RMSE: 0.0917\n",
            "Epoch [944/1000], Loss: 0.0060, RMSE: 0.0775\n",
            "Epoch [945/1000], Loss: 0.0081, RMSE: 0.0898\n",
            "Epoch [946/1000], Loss: 0.0023, RMSE: 0.0479\n",
            "Epoch [947/1000], Loss: 0.0061, RMSE: 0.0780\n",
            "Epoch [948/1000], Loss: 0.0067, RMSE: 0.0818\n",
            "Epoch [949/1000], Loss: 0.0058, RMSE: 0.0761\n",
            "Epoch [950/1000], Loss: 0.0058, RMSE: 0.0765\n",
            "Epoch [951/1000], Loss: 0.0128, RMSE: 0.1130\n",
            "Epoch [952/1000], Loss: 0.0105, RMSE: 0.1027\n",
            "Epoch [953/1000], Loss: 0.0111, RMSE: 0.1055\n",
            "Epoch [954/1000], Loss: 0.0073, RMSE: 0.0855\n",
            "Epoch [955/1000], Loss: 0.0051, RMSE: 0.0717\n",
            "Epoch [956/1000], Loss: 0.0099, RMSE: 0.0996\n",
            "Epoch [957/1000], Loss: 0.0090, RMSE: 0.0949\n",
            "Epoch [958/1000], Loss: 0.0061, RMSE: 0.0782\n",
            "Epoch [959/1000], Loss: 0.0090, RMSE: 0.0946\n",
            "Epoch [960/1000], Loss: 0.0145, RMSE: 0.1206\n",
            "Epoch [961/1000], Loss: 0.0111, RMSE: 0.1053\n",
            "Epoch [962/1000], Loss: 0.0068, RMSE: 0.0826\n",
            "Epoch [963/1000], Loss: 0.0085, RMSE: 0.0920\n",
            "Epoch [964/1000], Loss: 0.0052, RMSE: 0.0723\n",
            "Epoch [965/1000], Loss: 0.0073, RMSE: 0.0854\n",
            "Epoch [966/1000], Loss: 0.0043, RMSE: 0.0656\n",
            "Epoch [967/1000], Loss: 0.0065, RMSE: 0.0804\n",
            "Epoch [968/1000], Loss: 0.0044, RMSE: 0.0661\n",
            "Epoch [969/1000], Loss: 0.0078, RMSE: 0.0885\n",
            "Epoch [970/1000], Loss: 0.0117, RMSE: 0.1081\n",
            "Epoch [971/1000], Loss: 0.0107, RMSE: 0.1035\n",
            "Epoch [972/1000], Loss: 0.0076, RMSE: 0.0873\n",
            "Epoch [973/1000], Loss: 0.0031, RMSE: 0.0561\n",
            "Epoch [974/1000], Loss: 0.0030, RMSE: 0.0547\n",
            "Epoch [975/1000], Loss: 0.0085, RMSE: 0.0923\n",
            "Epoch [976/1000], Loss: 0.0170, RMSE: 0.1304\n",
            "Epoch [977/1000], Loss: 0.0182, RMSE: 0.1348\n",
            "Epoch [978/1000], Loss: 0.0057, RMSE: 0.0755\n",
            "Epoch [979/1000], Loss: 0.0106, RMSE: 0.1030\n",
            "Epoch [980/1000], Loss: 0.0110, RMSE: 0.1050\n",
            "Epoch [981/1000], Loss: 0.0080, RMSE: 0.0897\n",
            "Epoch [982/1000], Loss: 0.0073, RMSE: 0.0855\n",
            "Epoch [983/1000], Loss: 0.0072, RMSE: 0.0850\n",
            "Epoch [984/1000], Loss: 0.0114, RMSE: 0.1068\n",
            "Epoch [985/1000], Loss: 0.0325, RMSE: 0.1802\n",
            "Epoch [986/1000], Loss: 0.0048, RMSE: 0.0696\n",
            "Epoch [987/1000], Loss: 0.0043, RMSE: 0.0653\n",
            "Epoch [988/1000], Loss: 0.0078, RMSE: 0.0881\n",
            "Epoch [989/1000], Loss: 0.0193, RMSE: 0.1387\n",
            "Epoch [990/1000], Loss: 0.0078, RMSE: 0.0885\n",
            "Epoch [991/1000], Loss: 0.0061, RMSE: 0.0783\n",
            "Epoch [992/1000], Loss: 0.0139, RMSE: 0.1179\n",
            "Epoch [993/1000], Loss: 0.0047, RMSE: 0.0683\n",
            "Epoch [994/1000], Loss: 0.0119, RMSE: 0.1092\n",
            "Epoch [995/1000], Loss: 0.0061, RMSE: 0.0780\n",
            "Epoch [996/1000], Loss: 0.0070, RMSE: 0.0834\n",
            "Epoch [997/1000], Loss: 0.0053, RMSE: 0.0729\n",
            "Epoch [998/1000], Loss: 0.0086, RMSE: 0.0928\n",
            "Epoch [999/1000], Loss: 0.0107, RMSE: 0.1035\n",
            "Epoch [1000/1000], Loss: 0.0049, RMSE: 0.0702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_rmse = 0.0\n",
        "num_batches = len(test_data)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_data:\n",
        "        input_data = batch['state'].permute(0, 2, 1)  # 调整形状为 [batch_size, seq_len, feature_dim]\n",
        "        target_data = input_data[:, -1, :]  # 使用最后一个时间步作为目标\n",
        "\n",
        "        # 前向传播\n",
        "        output = model(input_data, None, input_data, None).squeeze()\n",
        "\n",
        "        # 计算 RMSE\n",
        "        rmse = torch.sqrt(torch.mean((output - target_data) ** 2))\n",
        "        total_rmse += rmse.item()\n",
        "\n",
        "avg_rmse = total_rmse / num_batches\n",
        "print(f\"Test RMSE: {avg_rmse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk-gsAZIiG4-",
        "outputId": "8cb985cc-7793-4df0-a9e8-4fe4cf3c7f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE: 0.1285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 测试"
      ],
      "metadata": {
        "id": "t_m_Vw_VoRAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the model parameters\n",
        "omega = 1\n",
        "b = 0.5\n",
        "\n",
        "# Generate the dataset S\n",
        "T = 10\n",
        "S = [(i, np.sin(omega * i + b)) for i in range(T)]\n",
        "\n",
        "# Generate the test data x[t] for an arbitrary t\n",
        "t = 7\n",
        "x_t = np.sin(omega * t + b)\n",
        "\n",
        "# Perform the different interpolation and extrapolation methods\n",
        "print(f\"In-domain interpolation: (omega, b) = (omega*, b*) and t ∈ [{min(x[0] for x in S)}, {max(x[0] for x in S)}]\")\n",
        "print(f\"In-domain extrapolation: (omega, b) = (omega*, b*) and t ∉ [{min(x[0] for x in S)}, {max(x[0] for x in S)}]\")\n",
        "print(f\"Out-of-domain interpolation: (omega, b) ≠ (omega*, b*) and t ∈ [{min(x[0] for x in S)}, {max(x[0] for x in S)}]\")\n",
        "print(f\"Out-of-domain extrapolation: (omega, b) ≠ (omega*, b*) and t ∉ [{min(x[0] for x in S)}, {max(x[0] for x in S)}]\")\n",
        "\n",
        "print(f\"\\nThe test data x[{t}] = {x_t:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLdwNcTjkZcV",
        "outputId": "1afdf448-172d-4caa-96f6-96a11f87bd28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-domain interpolation: (omega, b) = (omega*, b*) and t ∈ [0, 9]\n",
            "In-domain extrapolation: (omega, b) = (omega*, b*) and t ∉ [0, 9]\n",
            "Out-of-domain interpolation: (omega, b) ≠ (omega*, b*) and t ∈ [0, 9]\n",
            "Out-of-domain extrapolation: (omega, b) ≠ (omega*, b*) and t ∉ [0, 9]\n",
            "\n",
            "The test data x[7] = 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to tensors, but now with two features\n",
        "# For this example, the second feature will just be a constant (e.g., zero)\n",
        "x_enc = torch.tensor([[x[0], 0] for x in S], dtype=torch.float32).view(-1, 1, 2)  # Encoder input with 2 features\n",
        "y = torch.tensor([x[1] for x in S], dtype=torch.float32).view(-1, 1)  # Target sine values         # Target sine values\n",
        "\n",
        "# Time-mark encoding (if needed for position embedding, here just using a range)\n",
        "x_mark_enc = torch.arange(T).float().view(-1, 1)  # Example time-mark for encoder\n",
        "x_mark_dec = x_mark_enc  # Decoder time-mark can be the same (you might adjust this later)"
      ],
      "metadata": {
        "id": "UAKdkiyioTdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Here, for simplicity, using the same data for both encoder and decoder\n",
        "    # Normally, decoder input would differ (for example, using the previous time steps as input)\n",
        "    x_dec = x_enc  # Decoder input (in this case, same as encoder input)\n",
        "\n",
        "    # Forward pass\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)  # Forward pass through the model\n",
        "\n",
        "    # Compute loss\n",
        "    loss = criterion(output.squeeze(), y)  # Squeeze to remove unnecessary dimensions\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Optimizer step\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7nJpk8YLoUhm",
        "outputId": "a502f79e-1b5f-4004-aa41-27c1aeec4670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 1.3174\n",
            "Epoch [2/1000], Loss: 0.3911\n",
            "Epoch [3/1000], Loss: 0.6031\n",
            "Epoch [4/1000], Loss: 0.5784\n",
            "Epoch [5/1000], Loss: 0.5728\n",
            "Epoch [6/1000], Loss: 0.4065\n",
            "Epoch [7/1000], Loss: 0.3429\n",
            "Epoch [8/1000], Loss: 0.4135\n",
            "Epoch [9/1000], Loss: 0.4905\n",
            "Epoch [10/1000], Loss: 0.4355\n",
            "Epoch [11/1000], Loss: 0.5285\n",
            "Epoch [12/1000], Loss: 0.4377\n",
            "Epoch [13/1000], Loss: 0.4468\n",
            "Epoch [14/1000], Loss: 0.3897\n",
            "Epoch [15/1000], Loss: 0.4645\n",
            "Epoch [16/1000], Loss: 0.4588\n",
            "Epoch [17/1000], Loss: 0.4948\n",
            "Epoch [18/1000], Loss: 0.4193\n",
            "Epoch [19/1000], Loss: 0.4236\n",
            "Epoch [20/1000], Loss: 0.3715\n",
            "Epoch [21/1000], Loss: 0.3552\n",
            "Epoch [22/1000], Loss: 0.4852\n",
            "Epoch [23/1000], Loss: 0.4352\n",
            "Epoch [24/1000], Loss: 0.3071\n",
            "Epoch [25/1000], Loss: 0.3484\n",
            "Epoch [26/1000], Loss: 0.3890\n",
            "Epoch [27/1000], Loss: 0.4322\n",
            "Epoch [28/1000], Loss: 0.4415\n",
            "Epoch [29/1000], Loss: 0.3819\n",
            "Epoch [30/1000], Loss: 0.3557\n",
            "Epoch [31/1000], Loss: 0.4514\n",
            "Epoch [32/1000], Loss: 0.4059\n",
            "Epoch [33/1000], Loss: 0.4104\n",
            "Epoch [34/1000], Loss: 0.3492\n",
            "Epoch [35/1000], Loss: 0.4433\n",
            "Epoch [36/1000], Loss: 0.4659\n",
            "Epoch [37/1000], Loss: 0.3818\n",
            "Epoch [38/1000], Loss: 0.4179\n",
            "Epoch [39/1000], Loss: 0.3922\n",
            "Epoch [40/1000], Loss: 0.4268\n",
            "Epoch [41/1000], Loss: 0.4092\n",
            "Epoch [42/1000], Loss: 0.3969\n",
            "Epoch [43/1000], Loss: 0.3321\n",
            "Epoch [44/1000], Loss: 0.3776\n",
            "Epoch [45/1000], Loss: 0.3630\n",
            "Epoch [46/1000], Loss: 0.3439\n",
            "Epoch [47/1000], Loss: 0.4007\n",
            "Epoch [48/1000], Loss: 0.3936\n",
            "Epoch [49/1000], Loss: 0.3606\n",
            "Epoch [50/1000], Loss: 0.3285\n",
            "Epoch [51/1000], Loss: 0.3859\n",
            "Epoch [52/1000], Loss: 0.3658\n",
            "Epoch [53/1000], Loss: 0.3802\n",
            "Epoch [54/1000], Loss: 0.3603\n",
            "Epoch [55/1000], Loss: 0.3785\n",
            "Epoch [56/1000], Loss: 0.4969\n",
            "Epoch [57/1000], Loss: 0.3518\n",
            "Epoch [58/1000], Loss: 0.3357\n",
            "Epoch [59/1000], Loss: 0.3924\n",
            "Epoch [60/1000], Loss: 0.3516\n",
            "Epoch [61/1000], Loss: 0.4304\n",
            "Epoch [62/1000], Loss: 0.3763\n",
            "Epoch [63/1000], Loss: 0.3814\n",
            "Epoch [64/1000], Loss: 0.3123\n",
            "Epoch [65/1000], Loss: 0.4725\n",
            "Epoch [66/1000], Loss: 0.3161\n",
            "Epoch [67/1000], Loss: 0.3589\n",
            "Epoch [68/1000], Loss: 0.3172\n",
            "Epoch [69/1000], Loss: 0.2642\n",
            "Epoch [70/1000], Loss: 0.2878\n",
            "Epoch [71/1000], Loss: 0.3418\n",
            "Epoch [72/1000], Loss: 0.4137\n",
            "Epoch [73/1000], Loss: 0.2980\n",
            "Epoch [74/1000], Loss: 0.2965\n",
            "Epoch [75/1000], Loss: 0.2645\n",
            "Epoch [76/1000], Loss: 0.2975\n",
            "Epoch [77/1000], Loss: 0.2371\n",
            "Epoch [78/1000], Loss: 0.3622\n",
            "Epoch [79/1000], Loss: 0.3040\n",
            "Epoch [80/1000], Loss: 0.2949\n",
            "Epoch [81/1000], Loss: 0.2629\n",
            "Epoch [82/1000], Loss: 0.2631\n",
            "Epoch [83/1000], Loss: 0.2743\n",
            "Epoch [84/1000], Loss: 0.3054\n",
            "Epoch [85/1000], Loss: 0.3381\n",
            "Epoch [86/1000], Loss: 0.3039\n",
            "Epoch [87/1000], Loss: 0.3068\n",
            "Epoch [88/1000], Loss: 0.2331\n",
            "Epoch [89/1000], Loss: 0.2865\n",
            "Epoch [90/1000], Loss: 0.2895\n",
            "Epoch [91/1000], Loss: 0.2731\n",
            "Epoch [92/1000], Loss: 0.3104\n",
            "Epoch [93/1000], Loss: 0.2854\n",
            "Epoch [94/1000], Loss: 0.3209\n",
            "Epoch [95/1000], Loss: 0.2395\n",
            "Epoch [96/1000], Loss: 0.2789\n",
            "Epoch [97/1000], Loss: 0.2966\n",
            "Epoch [98/1000], Loss: 0.2151\n",
            "Epoch [99/1000], Loss: 0.2476\n",
            "Epoch [100/1000], Loss: 0.3287\n",
            "Epoch [101/1000], Loss: 0.4025\n",
            "Epoch [102/1000], Loss: 0.3522\n",
            "Epoch [103/1000], Loss: 0.2388\n",
            "Epoch [104/1000], Loss: 0.2164\n",
            "Epoch [105/1000], Loss: 0.1771\n",
            "Epoch [106/1000], Loss: 0.3272\n",
            "Epoch [107/1000], Loss: 0.2374\n",
            "Epoch [108/1000], Loss: 0.2984\n",
            "Epoch [109/1000], Loss: 0.2296\n",
            "Epoch [110/1000], Loss: 0.2335\n",
            "Epoch [111/1000], Loss: 0.2550\n",
            "Epoch [112/1000], Loss: 0.1863\n",
            "Epoch [113/1000], Loss: 0.1604\n",
            "Epoch [114/1000], Loss: 0.1579\n",
            "Epoch [115/1000], Loss: 0.2868\n",
            "Epoch [116/1000], Loss: 0.2399\n",
            "Epoch [117/1000], Loss: 0.1960\n",
            "Epoch [118/1000], Loss: 0.1497\n",
            "Epoch [119/1000], Loss: 0.3023\n",
            "Epoch [120/1000], Loss: 0.3003\n",
            "Epoch [121/1000], Loss: 0.1959\n",
            "Epoch [122/1000], Loss: 0.1437\n",
            "Epoch [123/1000], Loss: 0.1514\n",
            "Epoch [124/1000], Loss: 0.2072\n",
            "Epoch [125/1000], Loss: 0.2113\n",
            "Epoch [126/1000], Loss: 0.2242\n",
            "Epoch [127/1000], Loss: 0.0860\n",
            "Epoch [128/1000], Loss: 0.1411\n",
            "Epoch [129/1000], Loss: 0.1792\n",
            "Epoch [130/1000], Loss: 0.2710\n",
            "Epoch [131/1000], Loss: 0.1635\n",
            "Epoch [132/1000], Loss: 0.1927\n",
            "Epoch [133/1000], Loss: 0.1416\n",
            "Epoch [134/1000], Loss: 0.1826\n",
            "Epoch [135/1000], Loss: 0.1869\n",
            "Epoch [136/1000], Loss: 0.2605\n",
            "Epoch [137/1000], Loss: 0.1861\n",
            "Epoch [138/1000], Loss: 0.1654\n",
            "Epoch [139/1000], Loss: 0.2754\n",
            "Epoch [140/1000], Loss: 0.2254\n",
            "Epoch [141/1000], Loss: 0.2476\n",
            "Epoch [142/1000], Loss: 0.2768\n",
            "Epoch [143/1000], Loss: 0.2022\n",
            "Epoch [144/1000], Loss: 0.2958\n",
            "Epoch [145/1000], Loss: 0.2923\n",
            "Epoch [146/1000], Loss: 0.2563\n",
            "Epoch [147/1000], Loss: 0.1807\n",
            "Epoch [148/1000], Loss: 0.2452\n",
            "Epoch [149/1000], Loss: 0.2522\n",
            "Epoch [150/1000], Loss: 0.1802\n",
            "Epoch [151/1000], Loss: 0.1623\n",
            "Epoch [152/1000], Loss: 0.2459\n",
            "Epoch [153/1000], Loss: 0.1840\n",
            "Epoch [154/1000], Loss: 0.1244\n",
            "Epoch [155/1000], Loss: 0.2321\n",
            "Epoch [156/1000], Loss: 0.1771\n",
            "Epoch [157/1000], Loss: 0.1470\n",
            "Epoch [158/1000], Loss: 0.1486\n",
            "Epoch [159/1000], Loss: 0.1598\n",
            "Epoch [160/1000], Loss: 0.1389\n",
            "Epoch [161/1000], Loss: 0.1950\n",
            "Epoch [162/1000], Loss: 0.1377\n",
            "Epoch [163/1000], Loss: 0.1731\n",
            "Epoch [164/1000], Loss: 0.1439\n",
            "Epoch [165/1000], Loss: 0.1102\n",
            "Epoch [166/1000], Loss: 0.1355\n",
            "Epoch [167/1000], Loss: 0.2192\n",
            "Epoch [168/1000], Loss: 0.1102\n",
            "Epoch [169/1000], Loss: 0.1401\n",
            "Epoch [170/1000], Loss: 0.1105\n",
            "Epoch [171/1000], Loss: 0.2272\n",
            "Epoch [172/1000], Loss: 0.1559\n",
            "Epoch [173/1000], Loss: 0.2246\n",
            "Epoch [174/1000], Loss: 0.1933\n",
            "Epoch [175/1000], Loss: 0.1843\n",
            "Epoch [176/1000], Loss: 0.2358\n",
            "Epoch [177/1000], Loss: 0.2420\n",
            "Epoch [178/1000], Loss: 0.2679\n",
            "Epoch [179/1000], Loss: 0.2689\n",
            "Epoch [180/1000], Loss: 0.1792\n",
            "Epoch [181/1000], Loss: 0.1857\n",
            "Epoch [182/1000], Loss: 0.1901\n",
            "Epoch [183/1000], Loss: 0.1638\n",
            "Epoch [184/1000], Loss: 0.1498\n",
            "Epoch [185/1000], Loss: 0.2426\n",
            "Epoch [186/1000], Loss: 0.2097\n",
            "Epoch [187/1000], Loss: 0.2609\n",
            "Epoch [188/1000], Loss: 0.1709\n",
            "Epoch [189/1000], Loss: 0.1590\n",
            "Epoch [190/1000], Loss: 0.2074\n",
            "Epoch [191/1000], Loss: 0.1494\n",
            "Epoch [192/1000], Loss: 0.1605\n",
            "Epoch [193/1000], Loss: 0.1939\n",
            "Epoch [194/1000], Loss: 0.1908\n",
            "Epoch [195/1000], Loss: 0.1446\n",
            "Epoch [196/1000], Loss: 0.2286\n",
            "Epoch [197/1000], Loss: 0.2465\n",
            "Epoch [198/1000], Loss: 0.1777\n",
            "Epoch [199/1000], Loss: 0.1840\n",
            "Epoch [200/1000], Loss: 0.1099\n",
            "Epoch [201/1000], Loss: 0.1532\n",
            "Epoch [202/1000], Loss: 0.1050\n",
            "Epoch [203/1000], Loss: 0.2853\n",
            "Epoch [204/1000], Loss: 0.2423\n",
            "Epoch [205/1000], Loss: 0.1321\n",
            "Epoch [206/1000], Loss: 0.1462\n",
            "Epoch [207/1000], Loss: 0.1441\n",
            "Epoch [208/1000], Loss: 0.1152\n",
            "Epoch [209/1000], Loss: 0.1785\n",
            "Epoch [210/1000], Loss: 0.0982\n",
            "Epoch [211/1000], Loss: 0.1826\n",
            "Epoch [212/1000], Loss: 0.1227\n",
            "Epoch [213/1000], Loss: 0.1173\n",
            "Epoch [214/1000], Loss: 0.1503\n",
            "Epoch [215/1000], Loss: 0.1458\n",
            "Epoch [216/1000], Loss: 0.1707\n",
            "Epoch [217/1000], Loss: 0.1142\n",
            "Epoch [218/1000], Loss: 0.1270\n",
            "Epoch [219/1000], Loss: 0.1833\n",
            "Epoch [220/1000], Loss: 0.1450\n",
            "Epoch [221/1000], Loss: 0.1068\n",
            "Epoch [222/1000], Loss: 0.1977\n",
            "Epoch [223/1000], Loss: 0.1291\n",
            "Epoch [224/1000], Loss: 0.1719\n",
            "Epoch [225/1000], Loss: 0.1283\n",
            "Epoch [226/1000], Loss: 0.1190\n",
            "Epoch [227/1000], Loss: 0.1685\n",
            "Epoch [228/1000], Loss: 0.1291\n",
            "Epoch [229/1000], Loss: 0.1941\n",
            "Epoch [230/1000], Loss: 0.1313\n",
            "Epoch [231/1000], Loss: 0.1791\n",
            "Epoch [232/1000], Loss: 0.1489\n",
            "Epoch [233/1000], Loss: 0.1896\n",
            "Epoch [234/1000], Loss: 0.1716\n",
            "Epoch [235/1000], Loss: 0.2031\n",
            "Epoch [236/1000], Loss: 0.1602\n",
            "Epoch [237/1000], Loss: 0.1815\n",
            "Epoch [238/1000], Loss: 0.1767\n",
            "Epoch [239/1000], Loss: 0.1929\n",
            "Epoch [240/1000], Loss: 0.1829\n",
            "Epoch [241/1000], Loss: 0.1211\n",
            "Epoch [242/1000], Loss: 0.1453\n",
            "Epoch [243/1000], Loss: 0.1463\n",
            "Epoch [244/1000], Loss: 0.1267\n",
            "Epoch [245/1000], Loss: 0.1124\n",
            "Epoch [246/1000], Loss: 0.1988\n",
            "Epoch [247/1000], Loss: 0.1185\n",
            "Epoch [248/1000], Loss: 0.1961\n",
            "Epoch [249/1000], Loss: 0.1831\n",
            "Epoch [250/1000], Loss: 0.2421\n",
            "Epoch [251/1000], Loss: 0.0920\n",
            "Epoch [252/1000], Loss: 0.0940\n",
            "Epoch [253/1000], Loss: 0.1829\n",
            "Epoch [254/1000], Loss: 0.1657\n",
            "Epoch [255/1000], Loss: 0.1163\n",
            "Epoch [256/1000], Loss: 0.0993\n",
            "Epoch [257/1000], Loss: 0.1651\n",
            "Epoch [258/1000], Loss: 0.1568\n",
            "Epoch [259/1000], Loss: 0.0933\n",
            "Epoch [260/1000], Loss: 0.1004\n",
            "Epoch [261/1000], Loss: 0.1463\n",
            "Epoch [262/1000], Loss: 0.1065\n",
            "Epoch [263/1000], Loss: 0.1148\n",
            "Epoch [264/1000], Loss: 0.1295\n",
            "Epoch [265/1000], Loss: 0.1196\n",
            "Epoch [266/1000], Loss: 0.1425\n",
            "Epoch [267/1000], Loss: 0.1291\n",
            "Epoch [268/1000], Loss: 0.1790\n",
            "Epoch [269/1000], Loss: 0.1456\n",
            "Epoch [270/1000], Loss: 0.1196\n",
            "Epoch [271/1000], Loss: 0.1559\n",
            "Epoch [272/1000], Loss: 0.1541\n",
            "Epoch [273/1000], Loss: 0.1757\n",
            "Epoch [274/1000], Loss: 0.1171\n",
            "Epoch [275/1000], Loss: 0.1599\n",
            "Epoch [276/1000], Loss: 0.1271\n",
            "Epoch [277/1000], Loss: 0.1671\n",
            "Epoch [278/1000], Loss: 0.1322\n",
            "Epoch [279/1000], Loss: 0.1386\n",
            "Epoch [280/1000], Loss: 0.1458\n",
            "Epoch [281/1000], Loss: 0.1240\n",
            "Epoch [282/1000], Loss: 0.1425\n",
            "Epoch [283/1000], Loss: 0.1379\n",
            "Epoch [284/1000], Loss: 0.1866\n",
            "Epoch [285/1000], Loss: 0.1688\n",
            "Epoch [286/1000], Loss: 0.1009\n",
            "Epoch [287/1000], Loss: 0.1371\n",
            "Epoch [288/1000], Loss: 0.1017\n",
            "Epoch [289/1000], Loss: 0.1260\n",
            "Epoch [290/1000], Loss: 0.1182\n",
            "Epoch [291/1000], Loss: 0.1762\n",
            "Epoch [292/1000], Loss: 0.1440\n",
            "Epoch [293/1000], Loss: 0.1107\n",
            "Epoch [294/1000], Loss: 0.1480\n",
            "Epoch [295/1000], Loss: 0.1751\n",
            "Epoch [296/1000], Loss: 0.1044\n",
            "Epoch [297/1000], Loss: 0.1105\n",
            "Epoch [298/1000], Loss: 0.2407\n",
            "Epoch [299/1000], Loss: 0.1699\n",
            "Epoch [300/1000], Loss: 0.0714\n",
            "Epoch [301/1000], Loss: 0.1502\n",
            "Epoch [302/1000], Loss: 0.1009\n",
            "Epoch [303/1000], Loss: 0.1663\n",
            "Epoch [304/1000], Loss: 0.3936\n",
            "Epoch [305/1000], Loss: 0.1377\n",
            "Epoch [306/1000], Loss: 0.1867\n",
            "Epoch [307/1000], Loss: 0.2169\n",
            "Epoch [308/1000], Loss: 0.1670\n",
            "Epoch [309/1000], Loss: 0.1670\n",
            "Epoch [310/1000], Loss: 0.1562\n",
            "Epoch [311/1000], Loss: 0.1144\n",
            "Epoch [312/1000], Loss: 0.1463\n",
            "Epoch [313/1000], Loss: 0.1328\n",
            "Epoch [314/1000], Loss: 0.1064\n",
            "Epoch [315/1000], Loss: 0.1090\n",
            "Epoch [316/1000], Loss: 0.1949\n",
            "Epoch [317/1000], Loss: 0.0950\n",
            "Epoch [318/1000], Loss: 0.1335\n",
            "Epoch [319/1000], Loss: 0.1598\n",
            "Epoch [320/1000], Loss: 0.0957\n",
            "Epoch [321/1000], Loss: 0.1283\n",
            "Epoch [322/1000], Loss: 0.0847\n",
            "Epoch [323/1000], Loss: 0.1857\n",
            "Epoch [324/1000], Loss: 0.0918\n",
            "Epoch [325/1000], Loss: 0.1294\n",
            "Epoch [326/1000], Loss: 0.1997\n",
            "Epoch [327/1000], Loss: 0.1383\n",
            "Epoch [328/1000], Loss: 0.1095\n",
            "Epoch [329/1000], Loss: 0.1237\n",
            "Epoch [330/1000], Loss: 0.1188\n",
            "Epoch [331/1000], Loss: 0.2783\n",
            "Epoch [332/1000], Loss: 0.1898\n",
            "Epoch [333/1000], Loss: 0.1079\n",
            "Epoch [334/1000], Loss: 0.2024\n",
            "Epoch [335/1000], Loss: 0.1898\n",
            "Epoch [336/1000], Loss: 0.1426\n",
            "Epoch [337/1000], Loss: 0.0925\n",
            "Epoch [338/1000], Loss: 0.1778\n",
            "Epoch [339/1000], Loss: 0.1575\n",
            "Epoch [340/1000], Loss: 0.0890\n",
            "Epoch [341/1000], Loss: 0.1352\n",
            "Epoch [342/1000], Loss: 0.2462\n",
            "Epoch [343/1000], Loss: 0.1430\n",
            "Epoch [344/1000], Loss: 0.1118\n",
            "Epoch [345/1000], Loss: 0.1659\n",
            "Epoch [346/1000], Loss: 0.1609\n",
            "Epoch [347/1000], Loss: 0.1618\n",
            "Epoch [348/1000], Loss: 0.1056\n",
            "Epoch [349/1000], Loss: 0.1380\n",
            "Epoch [350/1000], Loss: 0.1651\n",
            "Epoch [351/1000], Loss: 0.0861\n",
            "Epoch [352/1000], Loss: 0.1198\n",
            "Epoch [353/1000], Loss: 0.1100\n",
            "Epoch [354/1000], Loss: 0.1436\n",
            "Epoch [355/1000], Loss: 0.0973\n",
            "Epoch [356/1000], Loss: 0.1384\n",
            "Epoch [357/1000], Loss: 0.1647\n",
            "Epoch [358/1000], Loss: 0.1021\n",
            "Epoch [359/1000], Loss: 0.1553\n",
            "Epoch [360/1000], Loss: 0.1834\n",
            "Epoch [361/1000], Loss: 0.1708\n",
            "Epoch [362/1000], Loss: 0.1001\n",
            "Epoch [363/1000], Loss: 0.1142\n",
            "Epoch [364/1000], Loss: 0.2748\n",
            "Epoch [365/1000], Loss: 0.0989\n",
            "Epoch [366/1000], Loss: 0.1431\n",
            "Epoch [367/1000], Loss: 0.1114\n",
            "Epoch [368/1000], Loss: 0.1413\n",
            "Epoch [369/1000], Loss: 0.1501\n",
            "Epoch [370/1000], Loss: 0.1023\n",
            "Epoch [371/1000], Loss: 0.1521\n",
            "Epoch [372/1000], Loss: 0.1728\n",
            "Epoch [373/1000], Loss: 0.1081\n",
            "Epoch [374/1000], Loss: 0.1071\n",
            "Epoch [375/1000], Loss: 0.1813\n",
            "Epoch [376/1000], Loss: 0.1197\n",
            "Epoch [377/1000], Loss: 0.1026\n",
            "Epoch [378/1000], Loss: 0.0798\n",
            "Epoch [379/1000], Loss: 0.1386\n",
            "Epoch [380/1000], Loss: 0.1425\n",
            "Epoch [381/1000], Loss: 0.1231\n",
            "Epoch [382/1000], Loss: 0.1142\n",
            "Epoch [383/1000], Loss: 0.1140\n",
            "Epoch [384/1000], Loss: 0.1339\n",
            "Epoch [385/1000], Loss: 0.1103\n",
            "Epoch [386/1000], Loss: 0.1238\n",
            "Epoch [387/1000], Loss: 0.1190\n",
            "Epoch [388/1000], Loss: 0.1068\n",
            "Epoch [389/1000], Loss: 0.1591\n",
            "Epoch [390/1000], Loss: 0.1121\n",
            "Epoch [391/1000], Loss: 0.1111\n",
            "Epoch [392/1000], Loss: 0.0874\n",
            "Epoch [393/1000], Loss: 0.1103\n",
            "Epoch [394/1000], Loss: 0.1333\n",
            "Epoch [395/1000], Loss: 0.1285\n",
            "Epoch [396/1000], Loss: 0.0901\n",
            "Epoch [397/1000], Loss: 0.1391\n",
            "Epoch [398/1000], Loss: 0.1131\n",
            "Epoch [399/1000], Loss: 0.1381\n",
            "Epoch [400/1000], Loss: 0.0968\n",
            "Epoch [401/1000], Loss: 0.1525\n",
            "Epoch [402/1000], Loss: 0.1094\n",
            "Epoch [403/1000], Loss: 0.0821\n",
            "Epoch [404/1000], Loss: 0.1038\n",
            "Epoch [405/1000], Loss: 0.1842\n",
            "Epoch [406/1000], Loss: 0.1069\n",
            "Epoch [407/1000], Loss: 0.1254\n",
            "Epoch [408/1000], Loss: 0.1771\n",
            "Epoch [409/1000], Loss: 0.1793\n",
            "Epoch [410/1000], Loss: 0.1163\n",
            "Epoch [411/1000], Loss: 0.1013\n",
            "Epoch [412/1000], Loss: 0.1067\n",
            "Epoch [413/1000], Loss: 0.1038\n",
            "Epoch [414/1000], Loss: 0.0916\n",
            "Epoch [415/1000], Loss: 0.1939\n",
            "Epoch [416/1000], Loss: 0.1248\n",
            "Epoch [417/1000], Loss: 0.0794\n",
            "Epoch [418/1000], Loss: 0.1187\n",
            "Epoch [419/1000], Loss: 0.1142\n",
            "Epoch [420/1000], Loss: 0.1253\n",
            "Epoch [421/1000], Loss: 0.0935\n",
            "Epoch [422/1000], Loss: 0.1185\n",
            "Epoch [423/1000], Loss: 0.1035\n",
            "Epoch [424/1000], Loss: 0.0940\n",
            "Epoch [425/1000], Loss: 0.1058\n",
            "Epoch [426/1000], Loss: 0.1362\n",
            "Epoch [427/1000], Loss: 0.1127\n",
            "Epoch [428/1000], Loss: 0.0743\n",
            "Epoch [429/1000], Loss: 0.1191\n",
            "Epoch [430/1000], Loss: 0.1000\n",
            "Epoch [431/1000], Loss: 0.1158\n",
            "Epoch [432/1000], Loss: 0.0718\n",
            "Epoch [433/1000], Loss: 0.1012\n",
            "Epoch [434/1000], Loss: 0.1392\n",
            "Epoch [435/1000], Loss: 0.1075\n",
            "Epoch [436/1000], Loss: 0.1390\n",
            "Epoch [437/1000], Loss: 0.1091\n",
            "Epoch [438/1000], Loss: 0.0802\n",
            "Epoch [439/1000], Loss: 0.1218\n",
            "Epoch [440/1000], Loss: 0.0951\n",
            "Epoch [441/1000], Loss: 0.0952\n",
            "Epoch [442/1000], Loss: 0.1395\n",
            "Epoch [443/1000], Loss: 0.0883\n",
            "Epoch [444/1000], Loss: 0.1399\n",
            "Epoch [445/1000], Loss: 0.1346\n",
            "Epoch [446/1000], Loss: 0.1372\n",
            "Epoch [447/1000], Loss: 0.0934\n",
            "Epoch [448/1000], Loss: 0.1370\n",
            "Epoch [449/1000], Loss: 0.1221\n",
            "Epoch [450/1000], Loss: 0.1291\n",
            "Epoch [451/1000], Loss: 0.1639\n",
            "Epoch [452/1000], Loss: 0.0812\n",
            "Epoch [453/1000], Loss: 0.1535\n",
            "Epoch [454/1000], Loss: 0.1617\n",
            "Epoch [455/1000], Loss: 0.1840\n",
            "Epoch [456/1000], Loss: 0.1091\n",
            "Epoch [457/1000], Loss: 0.1430\n",
            "Epoch [458/1000], Loss: 0.1284\n",
            "Epoch [459/1000], Loss: 0.0892\n",
            "Epoch [460/1000], Loss: 0.1053\n",
            "Epoch [461/1000], Loss: 0.1093\n",
            "Epoch [462/1000], Loss: 0.0705\n",
            "Epoch [463/1000], Loss: 0.1005\n",
            "Epoch [464/1000], Loss: 0.0793\n",
            "Epoch [465/1000], Loss: 0.1116\n",
            "Epoch [466/1000], Loss: 0.1137\n",
            "Epoch [467/1000], Loss: 0.0954\n",
            "Epoch [468/1000], Loss: 0.0859\n",
            "Epoch [469/1000], Loss: 0.1035\n",
            "Epoch [470/1000], Loss: 0.2127\n",
            "Epoch [471/1000], Loss: 0.0727\n",
            "Epoch [472/1000], Loss: 0.1280\n",
            "Epoch [473/1000], Loss: 0.1027\n",
            "Epoch [474/1000], Loss: 0.1057\n",
            "Epoch [475/1000], Loss: 0.0792\n",
            "Epoch [476/1000], Loss: 0.1301\n",
            "Epoch [477/1000], Loss: 0.1049\n",
            "Epoch [478/1000], Loss: 0.1064\n",
            "Epoch [479/1000], Loss: 0.1053\n",
            "Epoch [480/1000], Loss: 0.1242\n",
            "Epoch [481/1000], Loss: 0.0863\n",
            "Epoch [482/1000], Loss: 0.1274\n",
            "Epoch [483/1000], Loss: 0.1223\n",
            "Epoch [484/1000], Loss: 0.0992\n",
            "Epoch [485/1000], Loss: 0.1011\n",
            "Epoch [486/1000], Loss: 0.1719\n",
            "Epoch [487/1000], Loss: 0.0799\n",
            "Epoch [488/1000], Loss: 0.0942\n",
            "Epoch [489/1000], Loss: 0.1160\n",
            "Epoch [490/1000], Loss: 0.1076\n",
            "Epoch [491/1000], Loss: 0.1043\n",
            "Epoch [492/1000], Loss: 0.1311\n",
            "Epoch [493/1000], Loss: 0.1663\n",
            "Epoch [494/1000], Loss: 0.0723\n",
            "Epoch [495/1000], Loss: 0.1247\n",
            "Epoch [496/1000], Loss: 0.1121\n",
            "Epoch [497/1000], Loss: 0.0826\n",
            "Epoch [498/1000], Loss: 0.1311\n",
            "Epoch [499/1000], Loss: 0.1340\n",
            "Epoch [500/1000], Loss: 0.1069\n",
            "Epoch [501/1000], Loss: 0.0874\n",
            "Epoch [502/1000], Loss: 0.0791\n",
            "Epoch [503/1000], Loss: 0.1351\n",
            "Epoch [504/1000], Loss: 0.1218\n",
            "Epoch [505/1000], Loss: 0.0906\n",
            "Epoch [506/1000], Loss: 0.1195\n",
            "Epoch [507/1000], Loss: 0.1180\n",
            "Epoch [508/1000], Loss: 0.0915\n",
            "Epoch [509/1000], Loss: 0.1266\n",
            "Epoch [510/1000], Loss: 0.1121\n",
            "Epoch [511/1000], Loss: 0.1169\n",
            "Epoch [512/1000], Loss: 0.3950\n",
            "Epoch [513/1000], Loss: 0.0808\n",
            "Epoch [514/1000], Loss: 0.1016\n",
            "Epoch [515/1000], Loss: 0.1454\n",
            "Epoch [516/1000], Loss: 0.1786\n",
            "Epoch [517/1000], Loss: 0.1395\n",
            "Epoch [518/1000], Loss: 0.1733\n",
            "Epoch [519/1000], Loss: 0.1383\n",
            "Epoch [520/1000], Loss: 0.0987\n",
            "Epoch [521/1000], Loss: 0.1072\n",
            "Epoch [522/1000], Loss: 0.1953\n",
            "Epoch [523/1000], Loss: 0.1434\n",
            "Epoch [524/1000], Loss: 0.1221\n",
            "Epoch [525/1000], Loss: 0.1655\n",
            "Epoch [526/1000], Loss: 0.1471\n",
            "Epoch [527/1000], Loss: 0.1017\n",
            "Epoch [528/1000], Loss: 0.0757\n",
            "Epoch [529/1000], Loss: 0.1183\n",
            "Epoch [530/1000], Loss: 0.0967\n",
            "Epoch [531/1000], Loss: 0.1928\n",
            "Epoch [532/1000], Loss: 0.1686\n",
            "Epoch [533/1000], Loss: 0.2297\n",
            "Epoch [534/1000], Loss: 0.1543\n",
            "Epoch [535/1000], Loss: 0.1190\n",
            "Epoch [536/1000], Loss: 0.0979\n",
            "Epoch [537/1000], Loss: 0.2525\n",
            "Epoch [538/1000], Loss: 0.1447\n",
            "Epoch [539/1000], Loss: 0.0898\n",
            "Epoch [540/1000], Loss: 0.1687\n",
            "Epoch [541/1000], Loss: 0.1319\n",
            "Epoch [542/1000], Loss: 0.1122\n",
            "Epoch [543/1000], Loss: 0.0978\n",
            "Epoch [544/1000], Loss: 0.1144\n",
            "Epoch [545/1000], Loss: 0.1195\n",
            "Epoch [546/1000], Loss: 0.0885\n",
            "Epoch [547/1000], Loss: 0.1001\n",
            "Epoch [548/1000], Loss: 0.1286\n",
            "Epoch [549/1000], Loss: 0.1055\n",
            "Epoch [550/1000], Loss: 0.0759\n",
            "Epoch [551/1000], Loss: 0.1473\n",
            "Epoch [552/1000], Loss: 0.1159\n",
            "Epoch [553/1000], Loss: 0.0908\n",
            "Epoch [554/1000], Loss: 0.1351\n",
            "Epoch [555/1000], Loss: 0.1384\n",
            "Epoch [556/1000], Loss: 0.1359\n",
            "Epoch [557/1000], Loss: 0.0852\n",
            "Epoch [558/1000], Loss: 0.1081\n",
            "Epoch [559/1000], Loss: 0.1014\n",
            "Epoch [560/1000], Loss: 0.0885\n",
            "Epoch [561/1000], Loss: 0.0926\n",
            "Epoch [562/1000], Loss: 0.0837\n",
            "Epoch [563/1000], Loss: 0.1295\n",
            "Epoch [564/1000], Loss: 0.0764\n",
            "Epoch [565/1000], Loss: 0.1013\n",
            "Epoch [566/1000], Loss: 0.1340\n",
            "Epoch [567/1000], Loss: 0.1010\n",
            "Epoch [568/1000], Loss: 0.1188\n",
            "Epoch [569/1000], Loss: 0.0974\n",
            "Epoch [570/1000], Loss: 0.1082\n",
            "Epoch [571/1000], Loss: 0.1003\n",
            "Epoch [572/1000], Loss: 0.1033\n",
            "Epoch [573/1000], Loss: 0.1239\n",
            "Epoch [574/1000], Loss: 0.1078\n",
            "Epoch [575/1000], Loss: 0.1011\n",
            "Epoch [576/1000], Loss: 0.1166\n",
            "Epoch [577/1000], Loss: 0.0765\n",
            "Epoch [578/1000], Loss: 0.1106\n",
            "Epoch [579/1000], Loss: 0.1054\n",
            "Epoch [580/1000], Loss: 0.1039\n",
            "Epoch [581/1000], Loss: 0.1077\n",
            "Epoch [582/1000], Loss: 0.0975\n",
            "Epoch [583/1000], Loss: 0.0948\n",
            "Epoch [584/1000], Loss: 0.0887\n",
            "Epoch [585/1000], Loss: 0.0852\n",
            "Epoch [586/1000], Loss: 0.1451\n",
            "Epoch [587/1000], Loss: 0.0817\n",
            "Epoch [588/1000], Loss: 0.1117\n",
            "Epoch [589/1000], Loss: 0.1185\n",
            "Epoch [590/1000], Loss: 0.0969\n",
            "Epoch [591/1000], Loss: 0.1056\n",
            "Epoch [592/1000], Loss: 0.1398\n",
            "Epoch [593/1000], Loss: 0.1019\n",
            "Epoch [594/1000], Loss: 0.1030\n",
            "Epoch [595/1000], Loss: 0.0983\n",
            "Epoch [596/1000], Loss: 0.0872\n",
            "Epoch [597/1000], Loss: 0.0792\n",
            "Epoch [598/1000], Loss: 0.0884\n",
            "Epoch [599/1000], Loss: 0.1737\n",
            "Epoch [600/1000], Loss: 0.1138\n",
            "Epoch [601/1000], Loss: 0.1152\n",
            "Epoch [602/1000], Loss: 0.0863\n",
            "Epoch [603/1000], Loss: 0.1401\n",
            "Epoch [604/1000], Loss: 0.1029\n",
            "Epoch [605/1000], Loss: 0.0971\n",
            "Epoch [606/1000], Loss: 0.1202\n",
            "Epoch [607/1000], Loss: 0.0948\n",
            "Epoch [608/1000], Loss: 0.0909\n",
            "Epoch [609/1000], Loss: 0.1003\n",
            "Epoch [610/1000], Loss: 0.1093\n",
            "Epoch [611/1000], Loss: 0.1597\n",
            "Epoch [612/1000], Loss: 0.1225\n",
            "Epoch [613/1000], Loss: 0.0959\n",
            "Epoch [614/1000], Loss: 0.1464\n",
            "Epoch [615/1000], Loss: 0.0863\n",
            "Epoch [616/1000], Loss: 0.1011\n",
            "Epoch [617/1000], Loss: 0.1018\n",
            "Epoch [618/1000], Loss: 0.0981\n",
            "Epoch [619/1000], Loss: 0.1180\n",
            "Epoch [620/1000], Loss: 0.1030\n",
            "Epoch [621/1000], Loss: 0.0944\n",
            "Epoch [622/1000], Loss: 0.1231\n",
            "Epoch [623/1000], Loss: 0.1145\n",
            "Epoch [624/1000], Loss: 0.1338\n",
            "Epoch [625/1000], Loss: 0.0884\n",
            "Epoch [626/1000], Loss: 0.0972\n",
            "Epoch [627/1000], Loss: 0.0775\n",
            "Epoch [628/1000], Loss: 0.1560\n",
            "Epoch [629/1000], Loss: 0.1097\n",
            "Epoch [630/1000], Loss: 0.1214\n",
            "Epoch [631/1000], Loss: 0.1313\n",
            "Epoch [632/1000], Loss: 0.1004\n",
            "Epoch [633/1000], Loss: 0.0827\n",
            "Epoch [634/1000], Loss: 0.0959\n",
            "Epoch [635/1000], Loss: 0.1181\n",
            "Epoch [636/1000], Loss: 0.1846\n",
            "Epoch [637/1000], Loss: 0.0979\n",
            "Epoch [638/1000], Loss: 0.0966\n",
            "Epoch [639/1000], Loss: 0.1068\n",
            "Epoch [640/1000], Loss: 0.1639\n",
            "Epoch [641/1000], Loss: 0.0904\n",
            "Epoch [642/1000], Loss: 0.1288\n",
            "Epoch [643/1000], Loss: 0.1148\n",
            "Epoch [644/1000], Loss: 0.1072\n",
            "Epoch [645/1000], Loss: 0.0978\n",
            "Epoch [646/1000], Loss: 0.0956\n",
            "Epoch [647/1000], Loss: 0.0939\n",
            "Epoch [648/1000], Loss: 0.1486\n",
            "Epoch [649/1000], Loss: 0.1314\n",
            "Epoch [650/1000], Loss: 0.1124\n",
            "Epoch [651/1000], Loss: 0.1270\n",
            "Epoch [652/1000], Loss: 0.1435\n",
            "Epoch [653/1000], Loss: 0.1124\n",
            "Epoch [654/1000], Loss: 0.0899\n",
            "Epoch [655/1000], Loss: 0.0956\n",
            "Epoch [656/1000], Loss: 0.0870\n",
            "Epoch [657/1000], Loss: 0.0937\n",
            "Epoch [658/1000], Loss: 0.0844\n",
            "Epoch [659/1000], Loss: 0.1181\n",
            "Epoch [660/1000], Loss: 0.1312\n",
            "Epoch [661/1000], Loss: 0.1135\n",
            "Epoch [662/1000], Loss: 0.1103\n",
            "Epoch [663/1000], Loss: 0.0792\n",
            "Epoch [664/1000], Loss: 0.0981\n",
            "Epoch [665/1000], Loss: 0.1039\n",
            "Epoch [666/1000], Loss: 0.0924\n",
            "Epoch [667/1000], Loss: 0.0949\n",
            "Epoch [668/1000], Loss: 0.1006\n",
            "Epoch [669/1000], Loss: 0.1236\n",
            "Epoch [670/1000], Loss: 0.1082\n",
            "Epoch [671/1000], Loss: 0.1014\n",
            "Epoch [672/1000], Loss: 0.1349\n",
            "Epoch [673/1000], Loss: 0.1224\n",
            "Epoch [674/1000], Loss: 0.0907\n",
            "Epoch [675/1000], Loss: 0.1876\n",
            "Epoch [676/1000], Loss: 0.1410\n",
            "Epoch [677/1000], Loss: 0.0954\n",
            "Epoch [678/1000], Loss: 0.0767\n",
            "Epoch [679/1000], Loss: 0.1010\n",
            "Epoch [680/1000], Loss: 0.1523\n",
            "Epoch [681/1000], Loss: 0.1047\n",
            "Epoch [682/1000], Loss: 0.1810\n",
            "Epoch [683/1000], Loss: 0.1273\n",
            "Epoch [684/1000], Loss: 0.1014\n",
            "Epoch [685/1000], Loss: 0.0770\n",
            "Epoch [686/1000], Loss: 0.0802\n",
            "Epoch [687/1000], Loss: 0.1909\n",
            "Epoch [688/1000], Loss: 0.1572\n",
            "Epoch [689/1000], Loss: 0.0963\n",
            "Epoch [690/1000], Loss: 0.1478\n",
            "Epoch [691/1000], Loss: 0.1090\n",
            "Epoch [692/1000], Loss: 0.0807\n",
            "Epoch [693/1000], Loss: 0.0665\n",
            "Epoch [694/1000], Loss: 0.1066\n",
            "Epoch [695/1000], Loss: 0.1101\n",
            "Epoch [696/1000], Loss: 0.0730\n",
            "Epoch [697/1000], Loss: 0.1325\n",
            "Epoch [698/1000], Loss: 0.1317\n",
            "Epoch [699/1000], Loss: 0.0725\n",
            "Epoch [700/1000], Loss: 0.1070\n",
            "Epoch [701/1000], Loss: 0.0693\n",
            "Epoch [702/1000], Loss: 0.1081\n",
            "Epoch [703/1000], Loss: 0.1695\n",
            "Epoch [704/1000], Loss: 0.0869\n",
            "Epoch [705/1000], Loss: 0.0910\n",
            "Epoch [706/1000], Loss: 0.0920\n",
            "Epoch [707/1000], Loss: 0.1007\n",
            "Epoch [708/1000], Loss: 0.1049\n",
            "Epoch [709/1000], Loss: 0.0952\n",
            "Epoch [710/1000], Loss: 0.0791\n",
            "Epoch [711/1000], Loss: 0.1033\n",
            "Epoch [712/1000], Loss: 0.1211\n",
            "Epoch [713/1000], Loss: 0.0945\n",
            "Epoch [714/1000], Loss: 0.1470\n",
            "Epoch [715/1000], Loss: 0.1054\n",
            "Epoch [716/1000], Loss: 0.0907\n",
            "Epoch [717/1000], Loss: 0.0775\n",
            "Epoch [718/1000], Loss: 0.1343\n",
            "Epoch [719/1000], Loss: 0.1141\n",
            "Epoch [720/1000], Loss: 0.0960\n",
            "Epoch [721/1000], Loss: 0.1501\n",
            "Epoch [722/1000], Loss: 0.1166\n",
            "Epoch [723/1000], Loss: 0.0955\n",
            "Epoch [724/1000], Loss: 0.0894\n",
            "Epoch [725/1000], Loss: 0.0874\n",
            "Epoch [726/1000], Loss: 0.1345\n",
            "Epoch [727/1000], Loss: 0.0886\n",
            "Epoch [728/1000], Loss: 0.1000\n",
            "Epoch [729/1000], Loss: 0.0858\n",
            "Epoch [730/1000], Loss: 0.1077\n",
            "Epoch [731/1000], Loss: 0.1047\n",
            "Epoch [732/1000], Loss: 0.0840\n",
            "Epoch [733/1000], Loss: 0.1080\n",
            "Epoch [734/1000], Loss: 0.0657\n",
            "Epoch [735/1000], Loss: 0.0934\n",
            "Epoch [736/1000], Loss: 0.0809\n",
            "Epoch [737/1000], Loss: 0.1226\n",
            "Epoch [738/1000], Loss: 0.0929\n",
            "Epoch [739/1000], Loss: 0.1108\n",
            "Epoch [740/1000], Loss: 0.1158\n",
            "Epoch [741/1000], Loss: 0.1090\n",
            "Epoch [742/1000], Loss: 0.1488\n",
            "Epoch [743/1000], Loss: 0.1322\n",
            "Epoch [744/1000], Loss: 0.0923\n",
            "Epoch [745/1000], Loss: 0.0930\n",
            "Epoch [746/1000], Loss: 0.1016\n",
            "Epoch [747/1000], Loss: 0.0981\n",
            "Epoch [748/1000], Loss: 0.2242\n",
            "Epoch [749/1000], Loss: 0.0966\n",
            "Epoch [750/1000], Loss: 0.0835\n",
            "Epoch [751/1000], Loss: 0.0915\n",
            "Epoch [752/1000], Loss: 0.0827\n",
            "Epoch [753/1000], Loss: 0.0918\n",
            "Epoch [754/1000], Loss: 0.1156\n",
            "Epoch [755/1000], Loss: 0.0902\n",
            "Epoch [756/1000], Loss: 0.1109\n",
            "Epoch [757/1000], Loss: 0.0707\n",
            "Epoch [758/1000], Loss: 0.1008\n",
            "Epoch [759/1000], Loss: 0.1143\n",
            "Epoch [760/1000], Loss: 0.1795\n",
            "Epoch [761/1000], Loss: 0.0786\n",
            "Epoch [762/1000], Loss: 0.1066\n",
            "Epoch [763/1000], Loss: 0.0813\n",
            "Epoch [764/1000], Loss: 0.0653\n",
            "Epoch [765/1000], Loss: 0.1124\n",
            "Epoch [766/1000], Loss: 0.0851\n",
            "Epoch [767/1000], Loss: 0.0865\n",
            "Epoch [768/1000], Loss: 0.1224\n",
            "Epoch [769/1000], Loss: 0.0714\n",
            "Epoch [770/1000], Loss: 0.0849\n",
            "Epoch [771/1000], Loss: 0.0744\n",
            "Epoch [772/1000], Loss: 0.0964\n",
            "Epoch [773/1000], Loss: 0.0767\n",
            "Epoch [774/1000], Loss: 0.0977\n",
            "Epoch [775/1000], Loss: 0.0931\n",
            "Epoch [776/1000], Loss: 0.0726\n",
            "Epoch [777/1000], Loss: 0.0984\n",
            "Epoch [778/1000], Loss: 0.0931\n",
            "Epoch [779/1000], Loss: 0.0581\n",
            "Epoch [780/1000], Loss: 0.0795\n",
            "Epoch [781/1000], Loss: 0.0793\n",
            "Epoch [782/1000], Loss: 0.1153\n",
            "Epoch [783/1000], Loss: 0.1149\n",
            "Epoch [784/1000], Loss: 0.0947\n",
            "Epoch [785/1000], Loss: 0.0616\n",
            "Epoch [786/1000], Loss: 0.0565\n",
            "Epoch [787/1000], Loss: 0.0740\n",
            "Epoch [788/1000], Loss: 0.0602\n",
            "Epoch [789/1000], Loss: 0.0841\n",
            "Epoch [790/1000], Loss: 0.0946\n",
            "Epoch [791/1000], Loss: 0.1024\n",
            "Epoch [792/1000], Loss: 0.0656\n",
            "Epoch [793/1000], Loss: 0.1053\n",
            "Epoch [794/1000], Loss: 0.0874\n",
            "Epoch [795/1000], Loss: 0.1184\n",
            "Epoch [796/1000], Loss: 0.1032\n",
            "Epoch [797/1000], Loss: 0.1380\n",
            "Epoch [798/1000], Loss: 0.1662\n",
            "Epoch [799/1000], Loss: 0.0814\n",
            "Epoch [800/1000], Loss: 0.0979\n",
            "Epoch [801/1000], Loss: 0.0957\n",
            "Epoch [802/1000], Loss: 0.0584\n",
            "Epoch [803/1000], Loss: 0.0772\n",
            "Epoch [804/1000], Loss: 0.0647\n",
            "Epoch [805/1000], Loss: 0.0735\n",
            "Epoch [806/1000], Loss: 0.1387\n",
            "Epoch [807/1000], Loss: 0.0708\n",
            "Epoch [808/1000], Loss: 0.0841\n",
            "Epoch [809/1000], Loss: 0.1120\n",
            "Epoch [810/1000], Loss: 0.0797\n",
            "Epoch [811/1000], Loss: 0.1186\n",
            "Epoch [812/1000], Loss: 0.0823\n",
            "Epoch [813/1000], Loss: 0.0625\n",
            "Epoch [814/1000], Loss: 0.0768\n",
            "Epoch [815/1000], Loss: 0.0401\n",
            "Epoch [816/1000], Loss: 0.0634\n",
            "Epoch [817/1000], Loss: 0.1147\n",
            "Epoch [818/1000], Loss: 0.0518\n",
            "Epoch [819/1000], Loss: 0.0595\n",
            "Epoch [820/1000], Loss: 0.1186\n",
            "Epoch [821/1000], Loss: 0.0497\n",
            "Epoch [822/1000], Loss: 0.0546\n",
            "Epoch [823/1000], Loss: 0.0981\n",
            "Epoch [824/1000], Loss: 0.1087\n",
            "Epoch [825/1000], Loss: 0.0339\n",
            "Epoch [826/1000], Loss: 0.0571\n",
            "Epoch [827/1000], Loss: 0.1145\n",
            "Epoch [828/1000], Loss: 0.0868\n",
            "Epoch [829/1000], Loss: 0.0856\n",
            "Epoch [830/1000], Loss: 0.0691\n",
            "Epoch [831/1000], Loss: 0.0438\n",
            "Epoch [832/1000], Loss: 0.1429\n",
            "Epoch [833/1000], Loss: 0.1252\n",
            "Epoch [834/1000], Loss: 0.0730\n",
            "Epoch [835/1000], Loss: 0.0876\n",
            "Epoch [836/1000], Loss: 0.0908\n",
            "Epoch [837/1000], Loss: 0.1368\n",
            "Epoch [838/1000], Loss: 0.0813\n",
            "Epoch [839/1000], Loss: 0.1193\n",
            "Epoch [840/1000], Loss: 0.0835\n",
            "Epoch [841/1000], Loss: 0.0872\n",
            "Epoch [842/1000], Loss: 0.0919\n",
            "Epoch [843/1000], Loss: 0.0710\n",
            "Epoch [844/1000], Loss: 0.0709\n",
            "Epoch [845/1000], Loss: 0.1102\n",
            "Epoch [846/1000], Loss: 0.0962\n",
            "Epoch [847/1000], Loss: 0.0833\n",
            "Epoch [848/1000], Loss: 0.0759\n",
            "Epoch [849/1000], Loss: 0.0779\n",
            "Epoch [850/1000], Loss: 0.1540\n",
            "Epoch [851/1000], Loss: 0.1088\n",
            "Epoch [852/1000], Loss: 0.0597\n",
            "Epoch [853/1000], Loss: 0.0661\n",
            "Epoch [854/1000], Loss: 0.0727\n",
            "Epoch [855/1000], Loss: 0.1010\n",
            "Epoch [856/1000], Loss: 0.0752\n",
            "Epoch [857/1000], Loss: 0.1056\n",
            "Epoch [858/1000], Loss: 0.0780\n",
            "Epoch [859/1000], Loss: 0.0889\n",
            "Epoch [860/1000], Loss: 0.0976\n",
            "Epoch [861/1000], Loss: 0.1237\n",
            "Epoch [862/1000], Loss: 0.0820\n",
            "Epoch [863/1000], Loss: 0.0616\n",
            "Epoch [864/1000], Loss: 0.0746\n",
            "Epoch [865/1000], Loss: 0.1484\n",
            "Epoch [866/1000], Loss: 0.1435\n",
            "Epoch [867/1000], Loss: 0.1182\n",
            "Epoch [868/1000], Loss: 0.0949\n",
            "Epoch [869/1000], Loss: 0.1210\n",
            "Epoch [870/1000], Loss: 0.0620\n",
            "Epoch [871/1000], Loss: 0.1737\n",
            "Epoch [872/1000], Loss: 0.0905\n",
            "Epoch [873/1000], Loss: 0.1481\n",
            "Epoch [874/1000], Loss: 0.1146\n",
            "Epoch [875/1000], Loss: 0.1058\n",
            "Epoch [876/1000], Loss: 0.0975\n",
            "Epoch [877/1000], Loss: 0.0653\n",
            "Epoch [878/1000], Loss: 0.0950\n",
            "Epoch [879/1000], Loss: 0.1592\n",
            "Epoch [880/1000], Loss: 0.0904\n",
            "Epoch [881/1000], Loss: 0.1376\n",
            "Epoch [882/1000], Loss: 0.0728\n",
            "Epoch [883/1000], Loss: 0.1061\n",
            "Epoch [884/1000], Loss: 0.0565\n",
            "Epoch [885/1000], Loss: 0.0654\n",
            "Epoch [886/1000], Loss: 0.0852\n",
            "Epoch [887/1000], Loss: 0.0965\n",
            "Epoch [888/1000], Loss: 0.0908\n",
            "Epoch [889/1000], Loss: 0.0725\n",
            "Epoch [890/1000], Loss: 0.0789\n",
            "Epoch [891/1000], Loss: 0.0866\n",
            "Epoch [892/1000], Loss: 0.0918\n",
            "Epoch [893/1000], Loss: 0.0596\n",
            "Epoch [894/1000], Loss: 0.0512\n",
            "Epoch [895/1000], Loss: 0.0949\n",
            "Epoch [896/1000], Loss: 0.0626\n",
            "Epoch [897/1000], Loss: 0.1013\n",
            "Epoch [898/1000], Loss: 0.0831\n",
            "Epoch [899/1000], Loss: 0.1397\n",
            "Epoch [900/1000], Loss: 0.0976\n",
            "Epoch [901/1000], Loss: 0.0878\n",
            "Epoch [902/1000], Loss: 0.0954\n",
            "Epoch [903/1000], Loss: 0.0914\n",
            "Epoch [904/1000], Loss: 0.0970\n",
            "Epoch [905/1000], Loss: 0.0872\n",
            "Epoch [906/1000], Loss: 0.1018\n",
            "Epoch [907/1000], Loss: 0.0913\n",
            "Epoch [908/1000], Loss: 0.0552\n",
            "Epoch [909/1000], Loss: 0.0807\n",
            "Epoch [910/1000], Loss: 0.0528\n",
            "Epoch [911/1000], Loss: 0.0896\n",
            "Epoch [912/1000], Loss: 0.1428\n",
            "Epoch [913/1000], Loss: 0.1226\n",
            "Epoch [914/1000], Loss: 0.0523\n",
            "Epoch [915/1000], Loss: 0.0796\n",
            "Epoch [916/1000], Loss: 0.1149\n",
            "Epoch [917/1000], Loss: 0.1149\n",
            "Epoch [918/1000], Loss: 0.0878\n",
            "Epoch [919/1000], Loss: 0.1056\n",
            "Epoch [920/1000], Loss: 0.0877\n",
            "Epoch [921/1000], Loss: 0.0664\n",
            "Epoch [922/1000], Loss: 0.0550\n",
            "Epoch [923/1000], Loss: 0.1025\n",
            "Epoch [924/1000], Loss: 0.0965\n",
            "Epoch [925/1000], Loss: 0.0484\n",
            "Epoch [926/1000], Loss: 0.0789\n",
            "Epoch [927/1000], Loss: 0.0586\n",
            "Epoch [928/1000], Loss: 0.1637\n",
            "Epoch [929/1000], Loss: 0.0665\n",
            "Epoch [930/1000], Loss: 0.0870\n",
            "Epoch [931/1000], Loss: 0.0915\n",
            "Epoch [932/1000], Loss: 0.0500\n",
            "Epoch [933/1000], Loss: 0.0896\n",
            "Epoch [934/1000], Loss: 0.0820\n",
            "Epoch [935/1000], Loss: 0.0931\n",
            "Epoch [936/1000], Loss: 0.0859\n",
            "Epoch [937/1000], Loss: 0.0649\n",
            "Epoch [938/1000], Loss: 0.0473\n",
            "Epoch [939/1000], Loss: 0.0658\n",
            "Epoch [940/1000], Loss: 0.0541\n",
            "Epoch [941/1000], Loss: 0.0662\n",
            "Epoch [942/1000], Loss: 0.0833\n",
            "Epoch [943/1000], Loss: 0.0582\n",
            "Epoch [944/1000], Loss: 0.0781\n",
            "Epoch [945/1000], Loss: 0.0886\n",
            "Epoch [946/1000], Loss: 0.0814\n",
            "Epoch [947/1000], Loss: 0.0386\n",
            "Epoch [948/1000], Loss: 0.0638\n",
            "Epoch [949/1000], Loss: 0.0835\n",
            "Epoch [950/1000], Loss: 0.1048\n",
            "Epoch [951/1000], Loss: 0.0410\n",
            "Epoch [952/1000], Loss: 0.0616\n",
            "Epoch [953/1000], Loss: 0.0915\n",
            "Epoch [954/1000], Loss: 0.1147\n",
            "Epoch [955/1000], Loss: 0.0566\n",
            "Epoch [956/1000], Loss: 0.0677\n",
            "Epoch [957/1000], Loss: 0.0639\n",
            "Epoch [958/1000], Loss: 0.0754\n",
            "Epoch [959/1000], Loss: 0.0929\n",
            "Epoch [960/1000], Loss: 0.0749\n",
            "Epoch [961/1000], Loss: 0.0797\n",
            "Epoch [962/1000], Loss: 0.1782\n",
            "Epoch [963/1000], Loss: 0.0790\n",
            "Epoch [964/1000], Loss: 0.0748\n",
            "Epoch [965/1000], Loss: 0.0530\n",
            "Epoch [966/1000], Loss: 0.1002\n",
            "Epoch [967/1000], Loss: 0.0726\n",
            "Epoch [968/1000], Loss: 0.0581\n",
            "Epoch [969/1000], Loss: 0.0993\n",
            "Epoch [970/1000], Loss: 0.0407\n",
            "Epoch [971/1000], Loss: 0.0528\n",
            "Epoch [972/1000], Loss: 0.0824\n",
            "Epoch [973/1000], Loss: 0.0451\n",
            "Epoch [974/1000], Loss: 0.1098\n",
            "Epoch [975/1000], Loss: 0.0671\n",
            "Epoch [976/1000], Loss: 0.0942\n",
            "Epoch [977/1000], Loss: 0.0933\n",
            "Epoch [978/1000], Loss: 0.0463\n",
            "Epoch [979/1000], Loss: 0.0572\n",
            "Epoch [980/1000], Loss: 0.1162\n",
            "Epoch [981/1000], Loss: 0.1067\n",
            "Epoch [982/1000], Loss: 0.0555\n",
            "Epoch [983/1000], Loss: 0.0757\n",
            "Epoch [984/1000], Loss: 0.0819\n",
            "Epoch [985/1000], Loss: 0.1174\n",
            "Epoch [986/1000], Loss: 0.0734\n",
            "Epoch [987/1000], Loss: 0.0683\n",
            "Epoch [988/1000], Loss: 0.0952\n",
            "Epoch [989/1000], Loss: 0.0395\n",
            "Epoch [990/1000], Loss: 0.0644\n",
            "Epoch [991/1000], Loss: 0.1278\n",
            "Epoch [992/1000], Loss: 0.0603\n",
            "Epoch [993/1000], Loss: 0.1424\n",
            "Epoch [994/1000], Loss: 0.0496\n",
            "Epoch [995/1000], Loss: 0.0948\n",
            "Epoch [996/1000], Loss: 0.0810\n",
            "Epoch [997/1000], Loss: 0.0704\n",
            "Epoch [998/1000], Loss: 0.0310\n",
            "Epoch [999/1000], Loss: 0.0803\n",
            "Epoch [1000/1000], Loss: 0.0575\n"
          ]
        }
      ]
    }
  ]
}